{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SlEhbTibExsi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD QMINST DATASET **"
      ],
      "metadata": {
        "id": "GWU3RIgZODu-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Fashion MNIST dataset and preprocess\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = torchvision.datasets.QMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = torchvision.datasets.QMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "Pj-cYEjTJVbA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize the train dataset**"
      ],
      "metadata": {
        "id": "AJ-7040XOc32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels_map = {\n",
        "    0: \"0\",\n",
        "    1: \"1\",\n",
        "    2: \"2\",\n",
        "    3: \"3\",\n",
        "    4: \"4\",\n",
        "    5: \"5\",\n",
        "    6: \"6\",\n",
        "    7: \"7\",\n",
        "    8: \"8\",\n",
        "    9: \"9\",\n",
        "}\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "    img, label = train_dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "k7dpJthLH7fN",
        "outputId": "d4eaee4f-47e2-4b15-cc6c-3aae744f78b4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize test data**"
      ],
      "metadata": {
        "id": "oRWRQZ54Oj_m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Test DataSet\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(test_dataset), size=(1,)).item()\n",
        "    img, label = test_dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "Ex_NRuegKiAc",
        "outputId": "f7625d52-a3b3-44aa-b7f3-abc009be5cd4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 9 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn4AAAKSCAYAAABMVtaZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0sklEQVR4nO3debyVVbkH8HVkEFEQRcMBVByARJScCPMS6tU00hIcy0AvqJhiWiqpOBGkZSaac6VoeLPEgZDMORkUp0TBAUhFmURBUBQZhHP/uJ+8t1xrw4Z9zt7s9f1+Pv3zLJ/3fTye1/3rxbV2TW1tbW0AAKDqbVDuAQAAqB+CHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmBL8K8MILL4RDDz00NG/ePDRr1iwccsghYdKkSeUeC6rOsmXLwsCBA8M222wTNtpoo9ClS5fwyCOPlHssqDo+1ypXje/qLa+///3v4Wtf+1po06ZNOPXUU8OqVavCDTfcED744IPw7LPPhvbt25d7RKgaxx9/fBg5cmQ466yzwi677BKGDx8ennvuufDEE0+E/fffv9zjQVXwuVbZBL8y69GjR3j66afD9OnTQ8uWLUMIIcydOze0a9cuHHLIIeGee+4p84RQHZ599tnQpUuXcOWVV4ZzzjknhBDC0qVLw2677Ra+9KUvhaeeeqrME0J18LlW2fxRb5mNGzcu/Od//ufnD0cIIWy99dbh61//enjggQfCxx9/XMbpoHqMHDkyNGjQIJxyyimf15o0aRL69u0bnn766TBz5swyTgfVw+daZRP8ymzZsmVho402+kK9adOmYfny5WHKlCllmAqqz4svvhjatWsXmjdv/i/1fffdN4QQ/PdHUCI+1yqb4Fdm7du3DxMnTgwrV678vLZ8+fLwzDPPhBBCmD17drlGg6oyd+7csPXWW3+h/s/anDlz6nskqEo+1yqb4FdmP/jBD8K0adNC3759w6uvvhqmTJkSevfuHebOnRtCCOHTTz8t84RQHT799NOw4YYbfqHepEmTz9eBdedzrbIJfmXWv3//cMEFF4T//u//Dh07dgydOnUKb7zxRjjvvPNCCCFssskmZZ4QqsNGG20Uli1b9oX60qVLP18H1p3Ptcom+FWAoUOHhnnz5oVx48aFl19+OTz33HNh1apVIYQQ2rVrV+bpoDpsvfXWn79x+P/+Wdtmm23qeySoWj7XKlfDcg/A/9pss83+5RyxRx99NLRu3Tp06NChjFNB9ejcuXN44oknwkcfffQvGzz++d8dde7cuUyTQXXyuVaZvPGrQH/84x/Dc889F84666ywwQb+EUEpHHXUUWHlypXhlltu+by2bNmycNttt4UuXbqENm3alHE6qG4+1yqHA5zLbOzYsWHw4MHhkEMOCS1btgwTJ04Mt912Wzj44IPD6NGjQ8OGXspCqRxzzDHhvvvuC2effXbYeeedw+233x6effbZ8Nhjj4Vu3bqVezyoCj7XKpuffpltu+22oUGDBuHKK68MixcvDm3btg1DhgwJP/rRjzwcUGJ33HFHuOiii8Lvf//7sHDhwrD77ruHBx54QOiDEvK5Vtm88QMAyIQ/aAcAyITgBwCQCcEPACATgh8AQCYEPwCATAh+AACZEPwAADKxxicp1tTU1OUcUBaVeIylZ41q5FmD+rG6Z80bPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATgh8AQCYalnsAgFJo1qxZtH7RRRcle4444ohovX379smePn36ROt33HFHgemgMvXq1Sta33vvvZM9u+66a7Q+ceLEksy0LoYPHx6tz507t34HqWDe+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJmpqa2tr1+gvrKmp61mg3q3hr3+98qytnYEDB0brl19+eUnvs3z58mj9D3/4Q7Jn2LBh0fpLL71UipHWC561ute6detoffTo0cmejh07RusNG5b20I/Uz7rUvxeTJ0+O1nv06JHsmTVrVklnKLfV/Uy98QMAyITgBwCQCcEPACATgh8AQCYEPwCATAh+AACZcJxLmbRo0SK5dtZZZ0XrqS/TDiGEp59+Olr/3e9+l+x55plnkmu5cMTE+qXQz2b8+PHReteuXetqnDX28ccfR+uHHHJIsqcSvvC+lDxrpfGd73wnuXbJJZdE63vssUcdTfOv/v73vyfXXnnllWi90O/FTjvtFK1/7WtfK26wEMKCBQuSa6nPyVtuuSXZ8+abbxY9Q31xnAsAACEEwQ8AIBuCHwBAJgQ/AIBMCH4AAJmwq7eOXXzxxdF6nz59kj077rhj0fdZtGhRtN6gQYNkT7du3aL1SZMmFX3/9ZWdhuuXgw46KLn2yCOP1OMkpXHllVcm1wYOHFiPk9Q9z1pxtttuu2h9woQJyZ5GjRoV3TNixIho/dVXXy0wXdz777+fXPvggw+Kvl7z5s2j9a233jrZc+mll0brxx57bNH3//nPf55cO//884u+Xn2xqxcAgBCC4AcAkA3BDwAgE4IfAEAmBD8AgEwIfgAAmXCcSxFSP4O777472dOzZ89ofeXKlcmepUuXRuvDhw9P9qS+nPv+++9P9nz22WfR+oEHHpjsqTaOmKhMqWOIHnrooWRPKX9v33nnneTanXfeGa2vzfEOy5YtS64dfPDB0fr48eOLvk8l8Kx9UevWrZNro0ePjtb32GOPZM9jjz0Wrad+l6pR6giYm266Kdlz3HHHRevLly9P9pxwwgnR+siRIwtMVz8c5wIAQAhB8AMAyIbgBwCQCcEPACATgh8AQCYalnuA9UmnTp2i9V69eiV7Jk6cGK23bNky2ZPanThr1qwC08WtWrUquZb6ousDDjgg2fPEE08UPQMU67TTTovWS73jPLV79xvf+EayZ/r06dH6kiVLkj2XXXZZtL7hhhsme84777xofX3d1csXffvb306uFdq9mzJz5sx1GacqfPTRR9H6sGHDkj2pXb2NGzdO9qR28VfCrt7V8cYPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZMJxLv/moIMOSq59+umn0frUqVOTPW+99Va0ftZZZyV71ubYlv333z9a32+//ZI9jRo1itZPPvnkZI/jXKgPhY65KNa7776bXEsdG1PomU4ZOnRoci115FPnzp2TPa1atYrWmzRpkuxZunRpco3K86UvfanonsWLFyfXfvWrX63LOFUtdQxTCCE888wz0XqXLl2SPTvttNM6z1Qu3vgBAGRC8AMAyITgBwCQCcEPACATgh8AQCay3dW71157RevDhw9P9rRu3TpaL/Tl7P369YvWU7uI1lZq91Fq524hv/zlL9d1HFita6+9Nrn2H//xHyW7z6GHHppce/nll0t2n1LbZ599ovUdd9wx2fPqq6/W1TjUgbPPPrvonkcffTS5NmXKlHUZp6otXLgwuZb6d9Gdd95ZV+OUlTd+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBPZHuey6667RuupI1sKeeWVV5JrL774YtHXS+nTp09y7YorrojWly9fnuyZPXt2tD5t2rTiBoMCGjRoEK137tw52dO4ceOi73P//fdH65VwxMWECROi9T322CPZU1NTE63vu+++yR7Huaxfli5dmlzbZJNNovVCvzPt2rWL1v07vbAXXnghWl+xYkWyJ/V8Fvp3V6HP4/rkjR8AQCYEPwCATAh+AACZEPwAADIh+AEAZCLbXb2pHVOFrFq1KlofNGhQsueTTz6J1gvt/DnwwAOj9QsvvDDZ07Bh8f8o582bF61//PHHRV8LUjp06BCt77///iW9z+jRo6P11HNbnwYMGBCtn3jiicmejTfeOFrv0qVLsmf48OHFjEWZXXbZZcm1X//619H6jjvumOz561//Gq0ffPDByZ433ngjuZaL1K7nu+66K9nz/e9/P1o/++yzkz0///nPixusjnjjBwCQCcEPACATgh8AQCYEPwCATAh+AACZEPwAADKR7XEuvXr1KrrnF7/4RbT+8MMPJ3s22CCera+66qpkT79+/aL1Jk2aFJiueIW2qkOpHHPMMSW71tSpU5Nrd999d8nuA/Xh+uuvT67tt99+0fp3v/vdZE/btm2j9X/84x/JnhEjRkTrhY4emTJlSnKtUh1//PHJtX322Sda7927d9H3qampKbqnvnnjBwCQCcEPACATgh8AQCYEPwCATAh+AACZyHZX75gxY6L1gw46KNlz0kknReutW7dO9myxxRbR+q677prsKfXu3ZTx48fXy32ofo0aNUquDRw4sGT3WbJkSXLt448/Ltl9Su2EE06I1ps2bVrPk7C+6NOnT7ReaGf7D37wg2j9S1/6UrLne9/7XrRe6OSLzz77LLmWktrtevHFFyd7DjjggGj9oYceSvacfvrp0fouu+yS7GnQoEG0Xltbm+xJWZue+uaNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMhETe0a7j1eH754uBgdOnSI1v/6178me7bffvui75P68b7zzjvJnjvuuCNaHzRoULIn9c/npz/9abJn6NCh0fqyZcuSPdWmErfer4/PWqHjXEr5+/Tiiy8m1/baa6+S3afU/vSnP0XrRx11VNHXuuSSS5JrhZ73cvOs1b3U8WGnnXZasqdv377R+nbbbVeSmf4p9bOuxN+LNZE6PmqbbbYpuqfUVvcz9cYPACATgh8AQCYEPwCATAh+AACZEPwAADLRsNwDlMvrr78erZ9xxhnJnpNOOilaf++995I98+bNi9YL7b771re+Fa2vzQ60yZMnJ9dy2r1L3Sr0pe333HNPtF7oS+DXR61atUqudenSpejrzZ07N1r/3e9+V/S1KK927dpF602aNEn2vPzyy0XfZ/78+dF6oc+bW2+9NVrfZJNNir7/N77xjeRa9+7do/VS7+rdcssto/X999+/pPeZMmVKtF5fO3fXhTd+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBPZHueS8sADD6zVWimtzRe3p7z22msluxakFDqS4e23367HSepe6piLRx55JNnTpk2bou/z/PPPR+tz5swp+lqU12OPPRatN2rUKNlz4403RuuXXXZZSWb6p9mzZ5fsWlOnTk2uXXvttSW7TyH33ntvya71yiuvJNeuuOKKkt2nvnnjBwCQCcEPACATgh8AQCYEPwCATAh+AACZqKldw29IrqmpqetZstKgQYPk2vTp06P1tm3bJnvefffdaH377bdP9ixfvjy5lotSf0F4KVTbs9axY8doffLkyUVfa9GiRcm13/zmN9H6oEGDkj0rVqyI1rfaaqtkT//+/aP1iy++ONmTsmzZsuTawQcfHK2PHz++6PtUgpyftZkzZ0br2267bbJn7ty50XqvXr2SPRMnTixusArXuXPnaH3gwIHJnsMOOyxab968ebIn9Zl71llnJXsefPDB5Fq5re5Z88YPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZKJhuQfIVaGjWQqtpYwePTpad2QL5faPf/wjWn/uueeSPfvss0+03qJFi2TPueeeG61vvfXWyZ4JEyZE6yeffHKyZ88990yuFeuaa65Jrq2vx7bwRQcddFC0/vDDDyd7tttuu2h98ODByZ5+/fpF6++8806B6Upnyy23TK61bNkyWu/atWuy5/rrr4/WmzRpkuz55JNPovVHHnkk2XPiiSdG66kjddZ33vgBAGRC8AMAyITgBwCQCcEPACATgh8AQCZqatfwm7Or7Yvjy23nnXdOrqW+MLqQr3zlK9H6pEmTir5WTnL+4vhyK/RF60OGDInWGzRoUFfj1KlHH300Wv/Wt76V7Km2HfmetS9q3759cu3ee++N1ps3b57sWblyZbT+5JNPJntGjBgRrc+cOTPZc/7550frqc+hEELo1KlTtL42vxfz5s1LrqVmGz58eNH3WV+t7mfqjR8AQCYEPwCATAh+AACZEPwAADIh+AEAZELwAwDIhONcyqR3797Jtdtvvz1aL7SFfccdd4zWlyxZUtxgmXHERGW68cYbo/VTTz21nidZcwsXLkyu7bPPPtH6m2++WVfjVBzPWt0bO3ZstL7//vvX8yRflPpZL1u2LNnz9NNPR+tHHnlksmfRokVFzVWNHOcCAEAIQfADAMiG4AcAkAnBDwAgE4IfAEAm7Ootk9122y25Nnny5Gh9xowZyZ62bduu60hZstOwMm222WbR+rHHHpvs+fKXvxyt9+vXL9mz0UYbRet/+tOfkj2p5/C6665L9syaNSu5lgvPWt3bYYcdovUf//jHyZ7TTz+9jqb5VxdccEG0/sILLyR7Hnnkkboap6rZ1QsAQAhB8AMAyIbgBwCQCcEPACATgh8AQCYEPwCATDjOpUx23nnn5Nr06dOj9ZtuuinZc9ppp63zTDlyxATUD88a1A/HuQAAEEIQ/AAAsiH4AQBkQvADAMiE4AcAkImG5R6AL3rttdei9V//+tf1PAkAUE288QMAyITgBwCQCcEPACATgh8AQCYEPwCATAh+AACZqKldw2/O9mXWVCNfHA/1w7MG9WN1z5o3fgAAmRD8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZELwAwDIRE1tJX5zNgAAJeeNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyITgV4GGDh0aampqwm677VbuUaCqTJ8+PRx33HGhdevWoWnTpqFDhw5h8ODBYcmSJeUeDaqaz7XKUVNbW1tb7iH4P7NmzQrt27cPNTU1YYcddghTpkwp90hQFWbOnBl23333sOmmm4b+/fuHzTffPDz99NNh+PDh4YgjjgijRo0q94hQlXyuVZaG5R6Af3XOOeeEr371q2HlypVh/vz55R4Hqsbvf//7sGjRojB+/PjQsWPHEEIIp5xySli1alW44447wsKFC8Nmm21W5imh+vhcqyz+qLeCjB07NowcOTIMGzas3KNA1fnoo49CCCG0atXqX+pbb7112GCDDULjxo3LMRZUNZ9rlUfwqxArV64MAwYMCP369QudOnUq9zhQdbp37x5CCKFv375h0qRJYebMmeGPf/xjuPHGG8OZZ54ZNt544/IOCFXG51pl8ke9FeKmm24Kb7/9dnj00UfLPQpUpUMPPTT89Kc/DT/72c/Cn//858/rF154YRgyZEgZJ4Pq5HOtMgl+FWDBggXh4osvDhdddFHYcsstyz0OVK0ddtghdOvWLfTq1Su0bNkyjBkzJvzsZz8LW221VTjjjDPKPR5UDZ9rlUvwqwCDBg0Km2++eRgwYEC5R4Gqddddd4VTTjklTJs2LbRu3TqEEELPnj3DqlWrwsCBA8Pxxx8fWrZsWeYpoTr4XKtc/hu/Mps+fXq45ZZbwplnnhnmzJkTZsyYEWbMmBGWLl0aVqxYEWbMmBE++OCDco8J670bbrghfOUrX/k89P3TEUccEZYsWRJefPHFMk0G1cXnWmUT/Mps9uzZYdWqVeHMM88Mbdu2/fx/zzzzTJg2bVpo27ZtGDx4cLnHhPXevHnzwsqVK79QX7FiRQghhM8++6y+R4Kq5HOtsvmj3jLbbbfdwn333feF+qBBg8LixYvDNddcE3baaacyTAbVpV27duHhhx8O06ZNC+3atfu8/oc//CFssMEGYffddy/jdFA9fK5VNt/cUaG6d+8e5s+f74RzKJGxY8eGAw88MLRs2TKcccYZoWXLluGBBx4IDz74YOjXr1/4zW9+U+4Roar5XKsM3vgBWejWrVt46qmnwqWXXhpuuOGGsGDBgtC2bdswdOjQcN5555V7PIB64Y0fAEAmbO4AAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyscYHONfU1NTlHFAWlXiMpWeNauRZg/qxumfNGz8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEw0LPcAAJRGgwYNovU99tgj2dO7d+9o/aqrrkr2zJw5s7jBgIrhjR8AQCYEPwCATAh+AACZEPwAADIh+AEAZMKuXoAy2WCD9P/33nfffaP1o48+OtnTvXv3aL1FixbJnn79+kXrdu7mbZtttkmuDR48OFrfeOONkz2DBg2K1t94443iBqsDjRs3jtZXrVqV7Pnss8/qapw6540fAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyITjXPiC66+/PrmW2q5/4okn1tE0sH4odDTLUUcdFa0fdNBByZ6TTz45Wn/zzTeTPTvuuGO0PmbMmGTPuHHjkmvk6+qrr06upX6fCx1xcvPNN0fr9XWcy+67755cGzt2bLRe6Fm75JJLovXRo0cXN1gZeOMHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJmwq3c9ssceeyTXampqovWXXnop2dOqVato/bvf/W6y5/zzz0+uQbXYYYcdkmvHHHNMtN6rV69kzz777BOt/+Mf/0j2fPOb34zWv/3tbyd7Tj311Gh9xIgRyZ71+cvmWXdbbrlltN65c+dkz/z586P1gQMHJnv+9re/FTPWWkvNXejkidSO/Pvuuy/Z8+GHHxYzVkXxxg8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkwnEu65Ff/epXybUDDzwwWt95552TPVdddVW03qJFi2TPjBkzkmtQThtttFG0fvLJJyd7Ukc8FDo6afHixdF6oWfjv/7rv6L1+++/P9lz2mmnRet9+vRJ9gwePDhav/fee5M9VL8GDRok1/r27Rutt2vXLtmzYsWKaH3mzJnFDbaWmjRpklw799xzo/VCx5SNHz8+Wk99RoYQwpIlS5Jrlc4bPwCATAh+AACZEPwAADIh+AEAZELwAwDIhF29FejrX/96tL7vvvsme1auXBmtF/ri+NSXvY8aNSrZ8/jjjyfXoJyGDx8erR999NFFX2vSpEnJtVNOOSVaf/7555M9jRo1itZPOumkZM/QoUOj9UcffTTZc/XVV0frqV2Y5KF169bJtcsvv7zo602ePDlaf+yxx4q+1to4/vjjk2uFdu+mXHnlldH6+rxztxBv/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmHOdSJnvuuWdybdiwYdH6Jptskuy57rrrovW1Ocri5ptvTq4tX7686OtBqbRo0SK5ljoG6f333y+6Z+rUqcme2tra5FrKmWeeGa2njpEIIX1sS9++fZM9H374YXGDUVUaNox/pPfu3buk97n//vtLer1iDRgwoKTXK3R8UzXyxg8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMmFXbx1L7d797W9/m+zp3LlztD5x4sRkT2q34957753sSe3Mqq8v2oZiNWvWLLm20UYbRetz585N9rz11lvReqGduxtsEP//y5deemmy57zzzovWX3jhhWRPz549o/WPP/442UPe+vfvH60PHjy46GvNnDkzuXbrrbcWfb1yW7RoUXJtxYoV9TdIBfDGDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGTCcS4l0KpVq+TaZZddFq1/5StfSfZ89NFH0XqhLfTXXntttL58+fJkz80331x0D5TTkiVLkmtvv/12tN6hQ4dkT+rL3keMGJHsueKKK6L13r17J3vGjBkTrR9zzDHJnk8//TS5BjG77rprya71m9/8Jrk2e/bskt2nkG9961vR+o477lj0tf785z8n11LHubRp0ybZU+i4m0rnjR8AQCYEPwCATAh+AACZEPwAADIh+AEAZKKmttC3kf//v7Cmpq5nWW/ddNNNybVTTz01Wp84cWKy57TTTovWL7744mTPkUceGa0PGjQo2TN06NDkWi7W8Ne/XnnW1k5q9+7f//73ZM+CBQui9WXLliV7dtppp2i90E7g8847L1qfO3dusqfaeNZKo1mzZsm1KVOmROvbbbdd0fc59thjk2upXeqdOnUq+j5t27ZNrt1www3ReosWLYq+z6xZs5JrCxcujNbHjRuX7Dn99NOLnqG+rO5Z88YPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZKJhuQeoBo0aNSq6Z+nSpcm1E088MVpPHdkSQghvvfVWtD5q1KhkT//+/aP1QtvrL7jggmh95cqVyR6oD6+//nq0ftlllyV7rrjiiqLv87Of/Sxav/DCC4u+FhQrdZxQCGt3bEtKoaPAfvzjH0fr++67b8nuX2qtW7dOrjVo0CBanz59el2NU1be+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJmpq1/Cbs9fHL7OuL3379k2u/fa3v63HSUrj8ccfT6594xvfiNY/++yzuhqnTvni+OqR2rU3YsSIZE+3bt2i9dmzZyd72rRpU9xghBA8a6XSuXPn5NqLL75Yf4NUqHfffTdanzx5crIndVrF888/X5KZ6tvqnjVv/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmHOdSAo0aNUquHXnkkdH6zTffnOxp0aJFtJ7aph5CCO+9915yLeWll16K1gcOHJjsmTt3btH3qWSOmKhMDRs2jNZ33nnnZM8f//jHaH3bbbdN9my++ebR+tKlS5M9TZs2Ta6R5lkrjdSzEUIIF110UbS+55571tU4a6xly5bReteuXUt6n0MOOSRaf+SRR0p6n0rmOBcAAEIIgh8AQDYEPwCATAh+AACZEPwAADKR3h7EGluxYkVyLbWTKbVzN4QQpk6dGq0feuihyZ4ZM2Yk16ASbbPNNsm1n//859F6jx49kj3XXXddtP7nP/852TNhwoRo/S9/+UuyB8rps88+S65dcskl9ThJcbp16xatP/nkk0Vf6/HHH0+ujRs3rujr5cYbPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJx7mUQOPGjZNrBx98cNHXGzRoULTuyBbWR4cddli0fssttyR7Xn755Wj9O9/5TrJn4sSJ0fovf/nLZE+jRo2i9cceeyzZAxSvY8eOJbtW6rinEEJYunRpye5TrbzxAwDIhOAHAJAJwQ8AIBOCHwBAJgQ/AIBM2NVbAueee25y7cgjj4zW77///mTPqFGj1nUkqFd9+/ZNrg0bNixaHzNmTLLnxBNPjNYL7djr3r17tN6/f/9kz7Rp06L1u+66K9kDFK9nz54lu9Z7771XsmvlyBs/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnHuRShcePG0fpxxx2X7EkdP3HRRRcle1asWFHcYFBPWrVqFa1fcMEFyZ5JkyZF63369En2LFu2LFpPHdkSQgh33nlntP7JJ58ke1LH0CxcuDDZA9SPxx9/PFp/9dVX63mS6uKNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkwq7eIqS+OH633XZL9lxzzTXR+pQpU0oxEtSr3r17R+utW7dO9vzgBz+I1lM7d0MI4Sc/+Um0fu655yZ7Fi1aVNT9QwhhwoQJyTWgOHvttVdyrVu3bkVfb9y4cdH68uXLi74W/8cbPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJx7n8m2bNmiXXLrzwwmj9tddeS/b84he/WOeZoFIUOq4h5fjjj4/Wr7/++mTP9ttvH63Pnj072dO1a9do/b333iswHVAq+++/f3KtcePGRV/vgw8+WJdxSPDGDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyYVfvvznuuOOSa9ttt120ft555yV75syZs84zQaW46aabovVCX8Deu3fvaP2TTz5J9owYMSJaP+ecc5I9CxYsSK4BdW+TTTYp6fU23njjkl6P/+WNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMhETW1tbe0a/YU1NXU9S0V4+OGHk2vbbrtttN69e/dkz/vvv7+uI1GH1vDXv17l8qyRF89a9WvTpk1ybciQIdH6/vvvn+z5+te/Hq3PmjWruMEys7pnzRs/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMhEw3IPUGkaNGiQXLv99tujdTt3AcjdzJkzk2t9+vSpx0koxBs/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkIma2jX85mxfZk018sXxUD88a1A/VveseeMHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEzU1FbiN2cDAFBy3vgBAGRC8AMAyITgBwCQCcEPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4VaOjQoaGmpibstttu5R4FqsZzzz0XzjjjjNCxY8ew8cYbh+222y4cc8wxYdq0aeUeDaqez7XKUVNbW1tb7iH4P7NmzQrt27cPNTU1YYcddghTpkwp90hQFY466qgwYcKEcPTRR4fdd989vPvuu+G6664LH3/8cZg4caIPJKgjPtcqi+BXYY477rjw/vvvh5UrV4b58+d7QKBEnnrqqbD33nuHxo0bf16bPn166NSpUzjqqKPCiBEjyjgdVC+fa5XFH/VWkLFjx4aRI0eGYcOGlXsUqDr77bffv4S+EELYZZddQseOHcNrr71WpqmguvlcqzyCX4VYuXJlGDBgQOjXr1/o1KlTuceBLNTW1oZ58+aFLbbYotyjQNXxuVaZGpZ7AP7XTTfdFN5+++3w6KOPlnsUyMadd94ZZs+eHQYPHlzuUaDq+FyrTN74VYAFCxaEiy++OFx00UVhyy23LPc4kIXXX389nH766aFr166hT58+5R4HqorPtcol+FWAQYMGhc033zwMGDCg3KNAFt59993Qo0ePsOmmm4aRI0eGBg0alHskqCo+1yqXP+ots+nTp4dbbrklDBs2LMyZM+fz+tKlS8OKFSvCjBkzQvPmzcPmm29eximhenz44YfhsMMOC4sWLQrjxo0L22yzTblHgqric62yOc6lzP72t7+FAw44oOBf88Mf/tCOKCiBpUuXhkMOOSS88MIL4dFHHw1du3Yt90hQdXyuVTZv/Mpst912C/fdd98X6oMGDQqLFy8O11xzTdhpp53KMBlUl5UrV4Zjjz02PP3002HUqFFCH9QRn2uVzRu/CtW9e3cHXUIJnXXWWeGaa64Jhx9+eDjmmGO+sH7CCSeUYSrIh8+1yuCNH5CFSZMmhRBCGD16dBg9evQX1gU/IAfe+AEAZMJxLgAAmRD8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATgh8AQCbW+ADnmpqaupwDyqISj7H0rFGNPGtQP1b3rHnjBwCQCcEPACATgh8AQCYEPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyETDcg8AVLdNN900Wt92222TPUcccUS0PmnSpGTPoYceGq0/+OCDyZ6HHnoouZa7Nm3aJNdmzpxZj5MApeSNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkoqa2trZ2jf7Cmpq6ngXq3Rr++ter9fFZ23333ZNrd955Z7TeoUOHZE+DBg3WeaZ/Wrx4cXLtb3/7W7T+05/+NNnz/PPPr+tIFWXfffeN1h977LFkT7NmzYq+j2dt/dKtW7fk2pNPPhmtF/pnfNhhh0Xr6+vO+tTPZ+zYsfU8yRet7lnzxg8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkomG5B6B89t5772j96quvTvakjrl4+OGHSzIT66ezzz47udaxY8d6nOSLCh09cvjhh0fr++23X7Jn2rRp0fqQIUOSPQ8++GByrdy22mqraH3jjTeu50koh+233z5av++++5I9qeNC5s2bl+yZP39+cYNVgP79+yfXhg4dGq3ffPPNyZ4LLrhgnWcqBW/8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATdvX+my222CK5tj7uSmratGly7e67747WN91002RPoTXy9fLLLyfXPvroo2i9efPmyZ6pU6dG6z169Ej2vPvuu8m1lJ49e0brl19+ebInteP3+uuvT/Z07do1Wi+0CxJKpdDnwE9+8pNofW3+Xf/Xv/41ufbCCy8Ufb1yK3RaQernc9JJJyV77OoFAKBeCX4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnHufyb0aNHJ9dmzZoVrR999NF1Nc4623LLLZNr2223XbSeOuZldWvk6+qrr06ujRo1Klq/4YYbkj2nnHJKtP7OO+8UN9hqjBgxIlpfsWJFsueuu+6K1nfYYYdkz49//ONoPXWURgghrFq1KrkGxbj22muTayeeeGLR13vyySej9bPOOqvoa1WCbt26ReuFjndLGTJkyLqOU+e88QMAyITgBwCQCcEPACATgh8AQCYEPwCATGS7q/fQQw+N1rt06ZLsadmyZbTerFmzZM/ixYuLG6zErrvuuqJ7Cu1shmK9+eab0XrqGawEkyZNKun1zjnnnGj90ksvTfYsWbKkpDNQHVI7UEMI4cwzz4zWe/bsWfR9Ujt3QwjhgAMOKPp6lSz1c9tss82SPYsWLYrWJ0+eXIqR6pQ3fgAAmRD8AAAyIfgBAGRC8AMAyITgBwCQCcEPACAT2R7nkjqapZAFCxZE6+U+siWE9JEyHTp0SPY88cQTRdUhF4cffni5R6g306ZNi9bHjBlTz5Pw/x122GHR+ogRI5I9qeNHVq1alex58MEHo/UTTjihwHTrn1133TW5ljruptDP7b333ovWx44dW9xgZeCNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkIttdvd/73vei9ZqammTPU089VVfjrLNWrVpF6zvttFOy5957743W58yZU5KZoNLtsMMO0foPf/jDkt7ngw8+iNYL7RqsL6+//nq0ntPO5nLZe++9k2u33XZbtL7pppsWfZ/Uzt0QQjjppJOi9Q8//LDo+1SyU089tdwjVAxv/AAAMiH4AQBkQvADAMiE4AcAkAnBDwAgE4IfAEAmqvo4l4MPPji5duCBB0brCxcuTPZcfvnl6zxTXenfv3+0Xltbm+xJHdvSvn37ZM/UqVOLGwzKrHnz5sm1+++/P1rfdtttSzpD6gvvly5dWtL7sH5J/Xs7hBC22GKLoq/35JNPRuup378Qqu/Ylvpyzz33lHuEteaNHwBAJgQ/AIBMCH4AAJkQ/AAAMiH4AQBkoqp39Xbo0CG51qhRo2h9/vz5yZ4ePXpE6y+99FKyZ8WKFdH6ueeem+xJ2WuvvZJru+yyS9HX+9WvfhWtF9ppaFcvlapp06bR+iuvvJLsKeXu3QkTJiTXHnvssZLdh/XPyJEjo/WePXuW9D733ntvtN67d+9kT2pn8Ze//OVkT01NTbRe6BSJtZG6z1/+8pdkT+pnMGDAgKLv89BDDyV7LrzwwuRapfPGDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGSipnYN91+ntjtXslNOOSW5duONN0brhf4+S71VPaWUW+XHjx+fXEsdKfPGG28kexYsWFD0DJWsvv6ZFmN9fNbqyzbbbJNcu+OOO6L1gw46qKQzzJgxI1o/4IADiu7JSc7P2qpVq4qqr60NNoi/y6m2+3z88cdFXyt13FOh+3Tt2jXZ88wzzxQ9Q31Z3bPmjR8AQCYEPwCATAh+AACZEPwAADIh+AEAZKJhuQeoS7fddltyLbWL54Ybbkj2PP/889H6nDlzkj0bbrhhUdcKIYSnnnoqWk99mXYIIRx++OHRemrnbgghPPvss8k1qGuNGzdOrg0dOjRa79u3b7KnRYsW0fra7CadMGFCcu373/9+tG7nLrlYtGhRtP7ee++V9D6pXdcbbbRRsqfQzv+U1N/P3Llzi77W+sAbPwCATAh+AACZEPwAADIh+AEAZELwAwDIhOAHAJCJmto1POvAF8eXX6EjYPbcc89o/atf/Wqyx3EueX9xfH1p3759tP7Nb34z2XPVVVfV1Tj/YsyYMdH6SSedlOyZP39+XY1T1XJ+1latWlVUfW2ljimbMmVKsufmm28u+j6TJ0+O1seOHVv0tdbGb3/72+TaiSeeWPT1rr/++mj9hz/8YdHXqgSre9a88QMAyITgBwCQCcEPACATgh8AQCYEPwCATDQs9wCsuUI7dSpxxxz5OP/885Nrl1xySbTeuHHjks7wySefROu33nprsudHP/pRtL5y5cqSzAQhhNC9e/dovVevXkVf65577kmu1deu2voycuTIaL1nz55FX+vJJ59Mrq2vu3fXljd+AACZEPwAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBOOc1mPtG3bNrm2YsWKaH3p0qV1NQ4Z2n777aP1ww47LNlTymNbFixYkFwbNGhQtD5q1KhkT/PmzYuqF7LVVlsV3TNz5szk2pw5c4q+HpUpdcxKtR2/sjZ23XXX5Frq2JZVq1YVfZ/U0TA58sYPACATgh8AQCYEPwCATAh+AACZEPwAADJhV2+VmDZtWrT+8ssv1/MkVLOJEydG661atarnSb6oc+fO0fpPfvKTZM+yZcui9UK7emtqaqL1tfkZFNrVe+utt0brqWc9hBA+++yzaP3uu+8ubjCoJ6eeemq5Rwg777xztL733nsne+666666GqfOeeMHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMuE4lyrhC6ipD1tttVW0XltbWy/3b9myZXKtvo6FSB3nUuhn8Omnn0brrVu3TvakjqFJHdkSQgiffPJJtO44F3LXq1ev5Nqvf/3raH2XXXapq3HKyhs/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiEXb0VaMstt4zWGzdunOx56qmn6moc+Nzvf//7aP3b3/52sqdZs2Z1NU5FGT9+fHIt9fM54ogjkj3jxo2L1t98883iBoMKltolX2htgw2Kf2dV6D5rc731WV5/twAAGRP8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATjnOpQE2bNo3WC21Hf+utt+pqHPhc7969o/UePXoke4YMGRKtd+jQIdnz0EMPFTdYid12223JtY8++ihanzx5crJn4cKF0frtt99e3GBQZWpra4teW7VqVdH3GTlyZNE91cobPwCATAh+AACZEPwAADIh+AEAZELwAwDIhF29Fejtt9+O1pctW5bsSX3Z+9VXX12SmaCQMWPGJNfGjh0brbdo0SLZM3PmzHUdCVgPvP/++0X3LFmyJLl28cUXR+ujR48u+j7Vyhs/AIBMCH4AAJkQ/AAAMiH4AQBkQvADAMiE4AcAkAnHuVSJDTfcsNwjQNTixYuLqgP5GDJkSHJt0aJF0fqMGTOSPQ888MA6TlT9vPEDAMiE4AcAkAnBDwAgE4IfAEAmBD8AgEzU1NbW1q7RX1hTU9ezsBrz589PrnXp0iVaf+ONN+pqnKqwhr/+9cqzRjXyrEH9WN2z5o0fAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyITjXMiaIyagfnjWoH44zgUAgBCC4AcAkA3BDwAgE4IfAEAmBD8AgEwIfgAAmRD8AAAyIfgBAGRC8AMAyITgBwCQCcEPACATgh8AQCZqaivxm7MBACg5b/wAADIh+AEAZELwAwDIhOAHAJAJwQ8AIBOCHwBAJgQ/AIBMCH4AAJkQ/AAAMvE/SpNQuei5twgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural Network**"
      ],
      "metadata": {
        "id": "xYOjW1zwOrpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the neural network architecture\n",
        "class initial_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(initial_model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "IDoWLwBKKuhi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training function and optimizer**"
      ],
      "metadata": {
        "id": "HXvbZgCFOyDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,train_loader,criterion,optimizer,activation_function=nn.ReLU(),epochs=5):\n",
        "# Train the neural network\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        #model.train()\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(train_loader, 0):\n",
        "            inputs, labels = data\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # print every 100 mini-batches\n",
        "                print(f'Epoch {epoch + 1}, Batch {i + 1}, Loss: {running_loss / 100}')\n",
        "                running_loss = 0.0\n",
        "\n",
        "    print('Finished Training')"
      ],
      "metadata": {
        "id": "aB7_GojxK2Qr"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Evaluation**"
      ],
      "metadata": {
        "id": "d4MpUw8jO7Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate function\n",
        "def evaluate(model,test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    predictions = []\n",
        "    actual_labels=[]\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_loader:\n",
        "            images, labels = data\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predictions.extend(predicted.numpy())  # Store predictions\n",
        "            actual_labels.extend(labels.numpy())    # Store actual labels\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy={ correct / total}\n",
        "    return accuracy,predictions, actual_labels"
      ],
      "metadata": {
        "id": "nrctNfu9K2Lf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate Initial Model**"
      ],
      "metadata": {
        "id": "v93r6RN_mMY_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the neural network\n",
        "model = initial_model()\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "#activation=nn.ReLU()\n",
        "train(model, train_loader, criterion,optimizer,epochs=5)\n",
        "accuracy, predictions,actual_labels = evaluate(model, test_loader)\n",
        "print(\"Initial Test Accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3K90Pa5l1ps",
        "outputId": "b6e79168-a93b-45ef-e23f-02b6ab787651"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.0319840988516809\n",
            "Epoch 1, Batch 200, Loss: 0.4467392176389694\n",
            "Epoch 1, Batch 300, Loss: 0.3775040802359581\n",
            "Epoch 1, Batch 400, Loss: 0.3456498070061207\n",
            "Epoch 1, Batch 500, Loss: 0.34298179477453233\n",
            "Epoch 1, Batch 600, Loss: 0.2924286349862814\n",
            "Epoch 1, Batch 700, Loss: 0.2829448081552982\n",
            "Epoch 1, Batch 800, Loss: 0.2559429433941841\n",
            "Epoch 1, Batch 900, Loss: 0.250639445707202\n",
            "Epoch 2, Batch 100, Loss: 0.210436523668468\n",
            "Epoch 2, Batch 200, Loss: 0.2293991031497717\n",
            "Epoch 2, Batch 300, Loss: 0.20344482887536286\n",
            "Epoch 2, Batch 400, Loss: 0.19761018861085178\n",
            "Epoch 2, Batch 500, Loss: 0.1858900086581707\n",
            "Epoch 2, Batch 600, Loss: 0.19098695758730172\n",
            "Epoch 2, Batch 700, Loss: 0.17943280283361673\n",
            "Epoch 2, Batch 800, Loss: 0.174819322116673\n",
            "Epoch 2, Batch 900, Loss: 0.16135591443628072\n",
            "Epoch 3, Batch 100, Loss: 0.1465078818798065\n",
            "Epoch 3, Batch 200, Loss: 0.14878889633342623\n",
            "Epoch 3, Batch 300, Loss: 0.15223964016884564\n",
            "Epoch 3, Batch 400, Loss: 0.13265572547912596\n",
            "Epoch 3, Batch 500, Loss: 0.15987535759806634\n",
            "Epoch 3, Batch 600, Loss: 0.1414765146560967\n",
            "Epoch 3, Batch 700, Loss: 0.13183442603796722\n",
            "Epoch 3, Batch 800, Loss: 0.1366760846786201\n",
            "Epoch 3, Batch 900, Loss: 0.14780036695301532\n",
            "Epoch 4, Batch 100, Loss: 0.11287102514877916\n",
            "Epoch 4, Batch 200, Loss: 0.12170093799009919\n",
            "Epoch 4, Batch 300, Loss: 0.10523945903405547\n",
            "Epoch 4, Batch 400, Loss: 0.12426558911800384\n",
            "Epoch 4, Batch 500, Loss: 0.11161355923861266\n",
            "Epoch 4, Batch 600, Loss: 0.10891700950451196\n",
            "Epoch 4, Batch 700, Loss: 0.09591409463435412\n",
            "Epoch 4, Batch 800, Loss: 0.11596860986202956\n",
            "Epoch 4, Batch 900, Loss: 0.1140571800339967\n",
            "Epoch 5, Batch 100, Loss: 0.09560770235955715\n",
            "Epoch 5, Batch 200, Loss: 0.09229003126733006\n",
            "Epoch 5, Batch 300, Loss: 0.09171203478239477\n",
            "Epoch 5, Batch 400, Loss: 0.11675812745466828\n",
            "Epoch 5, Batch 500, Loss: 0.09799650591798127\n",
            "Epoch 5, Batch 600, Loss: 0.09251915064640343\n",
            "Epoch 5, Batch 700, Loss: 0.09451228416059167\n",
            "Epoch 5, Batch 800, Loss: 0.08743719224818051\n",
            "Epoch 5, Batch 900, Loss: 0.10296608855947852\n",
            "Finished Training\n",
            "Initial Test Accuracy: {0.96055}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a specific test image\n",
        "index_to_visualize = 2  # Change this index based on your preference\n",
        "image, label = test_loader.dataset[index_to_visualize]\n",
        "\n",
        "# Reshape the image tensor to a 28x28 shape\n",
        "image = image.view(28, 28)\n",
        "\n",
        "# Convert the image tensor to a numpy array for visualization\n",
        "image_numpy = image.numpy()\n",
        "\n",
        "# Show the image\n",
        "plt.imshow(image_numpy, cmap='gray')\n",
        "plt.title(f'Predicted Label: {labels_map[predictions[index_to_visualize]]}, Actual Label: {labels_map[actual_labels[index_to_visualize]]}')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "0jC3DlqLXW7h",
        "outputId": "616462c5-a5e4-4080-f8e3-7826ff486533"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding another Dense layer of 128 nodes**"
      ],
      "metadata": {
        "id": "EDlfocSMPJp4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the neural network architecture\n",
        "class deep_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(deep_model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.fc2 = nn.Linear(128, 128)\n",
        "        self.fc3 = nn.Linear(128, 64)\n",
        "        self.fc4 = nn.Linear(64,10)\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x= torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "3j2DR2EfPI4Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the neural network\n",
        "model = deep_model()\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train(model, train_loader, criterion, optimizer, epochs=5)\n",
        "accuracy, predictions,actual_labels = evaluate(model, test_loader)\n",
        "print(\"Deep Model Test Accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df3AWL7Onpnm",
        "outputId": "81d37e05-937e-47fd-c1e2-c2288d919e16"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.0410084307193757\n",
            "Epoch 1, Batch 200, Loss: 0.4685035166144371\n",
            "Epoch 1, Batch 300, Loss: 0.40151229709386826\n",
            "Epoch 1, Batch 400, Loss: 0.3440864232182503\n",
            "Epoch 1, Batch 500, Loss: 0.32792375415563585\n",
            "Epoch 1, Batch 600, Loss: 0.30026283212006094\n",
            "Epoch 1, Batch 700, Loss: 0.26334729984402655\n",
            "Epoch 1, Batch 800, Loss: 0.23759450040757657\n",
            "Epoch 1, Batch 900, Loss: 0.2168851861730218\n",
            "Epoch 2, Batch 100, Loss: 0.20524736627936363\n",
            "Epoch 2, Batch 200, Loss: 0.18478294745087623\n",
            "Epoch 2, Batch 300, Loss: 0.18525116827338933\n",
            "Epoch 2, Batch 400, Loss: 0.1694878584332764\n",
            "Epoch 2, Batch 500, Loss: 0.16161260660737753\n",
            "Epoch 2, Batch 600, Loss: 0.17190438080579043\n",
            "Epoch 2, Batch 700, Loss: 0.1644401926919818\n",
            "Epoch 2, Batch 800, Loss: 0.15341778874397277\n",
            "Epoch 2, Batch 900, Loss: 0.15189529836177826\n",
            "Epoch 3, Batch 100, Loss: 0.11667697614058853\n",
            "Epoch 3, Batch 200, Loss: 0.14603453110903503\n",
            "Epoch 3, Batch 300, Loss: 0.11895181464031339\n",
            "Epoch 3, Batch 400, Loss: 0.12608040820807218\n",
            "Epoch 3, Batch 500, Loss: 0.13532015020027757\n",
            "Epoch 3, Batch 600, Loss: 0.11824627814814448\n",
            "Epoch 3, Batch 700, Loss: 0.1159404195845127\n",
            "Epoch 3, Batch 800, Loss: 0.10634011486545206\n",
            "Epoch 3, Batch 900, Loss: 0.1393962951377034\n",
            "Epoch 4, Batch 100, Loss: 0.10432626336812972\n",
            "Epoch 4, Batch 200, Loss: 0.10486689603887499\n",
            "Epoch 4, Batch 300, Loss: 0.11319740268401801\n",
            "Epoch 4, Batch 400, Loss: 0.1033750585373491\n",
            "Epoch 4, Batch 500, Loss: 0.09368516394402832\n",
            "Epoch 4, Batch 600, Loss: 0.11039514710195363\n",
            "Epoch 4, Batch 700, Loss: 0.09495858331210912\n",
            "Epoch 4, Batch 800, Loss: 0.0999107117857784\n",
            "Epoch 4, Batch 900, Loss: 0.10814009779132902\n",
            "Epoch 5, Batch 100, Loss: 0.0763302526064217\n",
            "Epoch 5, Batch 200, Loss: 0.08533282285556197\n",
            "Epoch 5, Batch 300, Loss: 0.09428646853193641\n",
            "Epoch 5, Batch 400, Loss: 0.09909932777285575\n",
            "Epoch 5, Batch 500, Loss: 0.08848441800102591\n",
            "Epoch 5, Batch 600, Loss: 0.09756541416514665\n",
            "Epoch 5, Batch 700, Loss: 0.08421997155062855\n",
            "Epoch 5, Batch 800, Loss: 0.08477741953916848\n",
            "Epoch 5, Batch 900, Loss: 0.08683028804138303\n",
            "Finished Training\n",
            "Deep Model Test Accuracy: {0.9683666666666667}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C_QKto6LnuQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Increase the current number of nodes in the layer to 256**"
      ],
      "metadata": {
        "id": "uuWQh3z4Ry-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the neural network architecture\n",
        "class wide_model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(wide_model, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 256)\n",
        "        self.fc2 = nn.Linear(256, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "oUjcvV0yRB29"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the neural network\n",
        "model = wide_model()\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train(model, train_loader, criterion, optimizer, epochs=5)\n",
        "accuracy, predictions,actual_labels = evaluate(model, test_loader)\n",
        "print(\"Wide Model Test Accuracy:\",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvU1fC4voIGd",
        "outputId": "a8bd92ac-7df6-43e7-905f-439d48a6ead6"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Batch 100, Loss: 0.9398327751457691\n",
            "Epoch 1, Batch 200, Loss: 0.44894114404916763\n",
            "Epoch 1, Batch 300, Loss: 0.3787138964235783\n",
            "Epoch 1, Batch 400, Loss: 0.34663759395480154\n",
            "Epoch 1, Batch 500, Loss: 0.29696712642908096\n",
            "Epoch 1, Batch 600, Loss: 0.2749520232528448\n",
            "Epoch 1, Batch 700, Loss: 0.2629319277405739\n",
            "Epoch 1, Batch 800, Loss: 0.24692145958542824\n",
            "Epoch 1, Batch 900, Loss: 0.20913100738078355\n",
            "Epoch 2, Batch 100, Loss: 0.20517863005399703\n",
            "Epoch 2, Batch 200, Loss: 0.18132423292845487\n",
            "Epoch 2, Batch 300, Loss: 0.16659764207899572\n",
            "Epoch 2, Batch 400, Loss: 0.17283860728144645\n",
            "Epoch 2, Batch 500, Loss: 0.16723256886005403\n",
            "Epoch 2, Batch 600, Loss: 0.16115056520327925\n",
            "Epoch 2, Batch 700, Loss: 0.15316321671009064\n",
            "Epoch 2, Batch 800, Loss: 0.1499151744134724\n",
            "Epoch 2, Batch 900, Loss: 0.13720235265791417\n",
            "Epoch 3, Batch 100, Loss: 0.12151058521121741\n",
            "Epoch 3, Batch 200, Loss: 0.1297267073020339\n",
            "Epoch 3, Batch 300, Loss: 0.12543296542018653\n",
            "Epoch 3, Batch 400, Loss: 0.11182395135983825\n",
            "Epoch 3, Batch 500, Loss: 0.11293823590502143\n",
            "Epoch 3, Batch 600, Loss: 0.1189470899105072\n",
            "Epoch 3, Batch 700, Loss: 0.11118461511097849\n",
            "Epoch 3, Batch 800, Loss: 0.10711568729951977\n",
            "Epoch 3, Batch 900, Loss: 0.1215483869239688\n",
            "Epoch 4, Batch 100, Loss: 0.09437209438532591\n",
            "Epoch 4, Batch 200, Loss: 0.09476661269553005\n",
            "Epoch 4, Batch 300, Loss: 0.09831499645486474\n",
            "Epoch 4, Batch 400, Loss: 0.09654346782714128\n",
            "Epoch 4, Batch 500, Loss: 0.10467121212743223\n",
            "Epoch 4, Batch 600, Loss: 0.08798392497934401\n",
            "Epoch 4, Batch 700, Loss: 0.09997832323424519\n",
            "Epoch 4, Batch 800, Loss: 0.10313668557442725\n",
            "Epoch 4, Batch 900, Loss: 0.09450835650786757\n",
            "Epoch 5, Batch 100, Loss: 0.08930653816089035\n",
            "Epoch 5, Batch 200, Loss: 0.07209452438168228\n",
            "Epoch 5, Batch 300, Loss: 0.08555924536660314\n",
            "Epoch 5, Batch 400, Loss: 0.086226538522169\n",
            "Epoch 5, Batch 500, Loss: 0.07599896569736302\n",
            "Epoch 5, Batch 600, Loss: 0.07378019027877598\n",
            "Epoch 5, Batch 700, Loss: 0.07661341797560453\n",
            "Epoch 5, Batch 800, Loss: 0.09620055653620511\n",
            "Epoch 5, Batch 900, Loss: 0.07950610220432282\n",
            "Finished Training\n",
            "Wide Model Test Accuracy: {0.9675166666666667}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initial Baseline accuracy of the Model was 96.05% and by adding a dense layer of 128 nodes accuracy had increased slightly to 96.83%,and with adding  by increasing the current number of nodes in the layer to 256 the accuracy was 96.75%."
      ],
      "metadata": {
        "id": "3XgxzMKXfZSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "zHW7bGoNgKXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Experiment with different optimizers, loss functions, dropout, and activation functions, and observe the change in performance as you tune these hyperparameters.**"
      ],
      "metadata": {
        "id": "6WfwPd-Kjkoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the neural network architecture\n",
        "class hypertuning(nn.Module):\n",
        "    def __init__(self,dropout_rate,activation):\n",
        "        super(hypertuning, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 128)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.activation = activation\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.view(-1, 28 * 28)\n",
        "        x =self.activation(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.activation(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ],
      "metadata": {
        "id": "3j2WLia-24ZB"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_list = list()\n",
        "criterion_list = list()\n",
        "activation_list = list()\n",
        "optimizer_list = list()\n",
        "dropout_list = list()\n",
        "accuracy_list=list()"
      ],
      "metadata": {
        "id": "bcgKMFGUFoWA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion_names = [nn.CrossEntropyLoss,nn.NLLLoss]\n",
        "optimizers = [optim.Adam, optim.SGD]\n",
        "activation= [nn.Sigmoid(),nn.ReLU(), nn.Tanh()]\n",
        "dropout_rates=[0.2,0.5]\n",
        "for dropout_rate in dropout_rates:\n",
        "  for activation_function  in activation:\n",
        "    for criterion_name in criterion_names:\n",
        "      for optimizer_name in optimizers:\n",
        "        model = hypertuning(dropout_rate,activation_function)\n",
        "        optimize = optimizer_name(model.parameters(), lr=0.001)\n",
        "        criterion=criterion_name()\n",
        "          # Train the model\n",
        "        train(model,train_loader,criterion,optimize,activation_function,epochs=5)\n",
        "\n",
        "          # Evaluate on the test set\n",
        "        test_accuracy, predictions,actual_labels = evaluate(model, test_loader)\n",
        "\n",
        "          # Report results\n",
        "        criterion_list.append(criterion_name.__name__)\n",
        "        optimizer_list.append(optimizer_name.__name__)\n",
        "        activation_list.append(activation_function.__class__.__name__)\n",
        "        dropout_list.append(dropout_rate)\n",
        "        accuracy_list.append(test_accuracy)\n",
        "\n",
        "\n",
        "hyperparam_list = pd.DataFrame({'criterion':criterion_list, 'optimizer':optimizer_list, 'activation':activation_list,'Dropout Rate':dropout_list, 'Accuracy':accuracy_list})\n",
        "hyperparam_sorted=hyperparam_list.sort_values(by='Accuracy', ascending=False, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kbav3WfSRBtN",
        "outputId": "5b9f1de8-09e7-4928-cacb-b8250b7662d2"
      },
      "execution_count": 55,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Batch 100, Loss: 1.98241619348526\n",
            "Epoch 1, Batch 200, Loss: 1.1586242723464966\n",
            "Epoch 1, Batch 300, Loss: 0.7574405658245087\n",
            "Epoch 1, Batch 400, Loss: 0.5706904631853104\n",
            "Epoch 1, Batch 500, Loss: 0.46871946692466737\n",
            "Epoch 1, Batch 600, Loss: 0.4350895076990128\n",
            "Epoch 1, Batch 700, Loss: 0.3556955824792385\n",
            "Epoch 1, Batch 800, Loss: 0.35932225942611695\n",
            "Epoch 1, Batch 900, Loss: 0.3352844862639904\n",
            "Epoch 2, Batch 100, Loss: 0.30862410575151444\n",
            "Epoch 2, Batch 200, Loss: 0.27736405603587627\n",
            "Epoch 2, Batch 300, Loss: 0.2661598752439022\n",
            "Epoch 2, Batch 400, Loss: 0.26180385835468767\n",
            "Epoch 2, Batch 500, Loss: 0.26071149796247484\n",
            "Epoch 2, Batch 600, Loss: 0.2650161398202181\n",
            "Epoch 2, Batch 700, Loss: 0.24261262387037277\n",
            "Epoch 2, Batch 800, Loss: 0.23540566362440585\n",
            "Epoch 2, Batch 900, Loss: 0.2329515041410923\n",
            "Epoch 3, Batch 100, Loss: 0.20802880808711052\n",
            "Epoch 3, Batch 200, Loss: 0.20196749426424504\n",
            "Epoch 3, Batch 300, Loss: 0.2056250737607479\n",
            "Epoch 3, Batch 400, Loss: 0.19717232778668403\n",
            "Epoch 3, Batch 500, Loss: 0.1912357696890831\n",
            "Epoch 3, Batch 600, Loss: 0.1913807614520192\n",
            "Epoch 3, Batch 700, Loss: 0.19073766343295573\n",
            "Epoch 3, Batch 800, Loss: 0.20215707633644342\n",
            "Epoch 3, Batch 900, Loss: 0.1956455533206463\n",
            "Epoch 4, Batch 100, Loss: 0.17494351856410503\n",
            "Epoch 4, Batch 200, Loss: 0.16680476371198893\n",
            "Epoch 4, Batch 300, Loss: 0.17855412874370813\n",
            "Epoch 4, Batch 400, Loss: 0.16539127327501774\n",
            "Epoch 4, Batch 500, Loss: 0.1732702881656587\n",
            "Epoch 4, Batch 600, Loss: 0.16164442915469407\n",
            "Epoch 4, Batch 700, Loss: 0.15776072543114425\n",
            "Epoch 4, Batch 800, Loss: 0.15872081980109215\n",
            "Epoch 4, Batch 900, Loss: 0.15117896296083927\n",
            "Epoch 5, Batch 100, Loss: 0.15205323534086346\n",
            "Epoch 5, Batch 200, Loss: 0.1515949378348887\n",
            "Epoch 5, Batch 300, Loss: 0.14526163216680288\n",
            "Epoch 5, Batch 400, Loss: 0.14527542114257813\n",
            "Epoch 5, Batch 500, Loss: 0.1467490128427744\n",
            "Epoch 5, Batch 600, Loss: 0.14122289948165417\n",
            "Epoch 5, Batch 700, Loss: 0.13571131628006697\n",
            "Epoch 5, Batch 800, Loss: 0.13505619836971164\n",
            "Epoch 5, Batch 900, Loss: 0.13846978317946196\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.3301119422912597\n",
            "Epoch 1, Batch 200, Loss: 2.3208895111083985\n",
            "Epoch 1, Batch 300, Loss: 2.30970504283905\n",
            "Epoch 1, Batch 400, Loss: 2.309013671875\n",
            "Epoch 1, Batch 500, Loss: 2.306276190280914\n",
            "Epoch 1, Batch 600, Loss: 2.3012625765800476\n",
            "Epoch 1, Batch 700, Loss: 2.3015235352516172\n",
            "Epoch 1, Batch 800, Loss: 2.2993836903572085\n",
            "Epoch 1, Batch 900, Loss: 2.298111095428467\n",
            "Epoch 2, Batch 100, Loss: 2.2973853731155396\n",
            "Epoch 2, Batch 200, Loss: 2.299165804386139\n",
            "Epoch 2, Batch 300, Loss: 2.298124396800995\n",
            "Epoch 2, Batch 400, Loss: 2.297123291492462\n",
            "Epoch 2, Batch 500, Loss: 2.2969604682922364\n",
            "Epoch 2, Batch 600, Loss: 2.2964055252075197\n",
            "Epoch 2, Batch 700, Loss: 2.296513876914978\n",
            "Epoch 2, Batch 800, Loss: 2.2948729681968687\n",
            "Epoch 2, Batch 900, Loss: 2.295948398113251\n",
            "Epoch 3, Batch 100, Loss: 2.296018943786621\n",
            "Epoch 3, Batch 200, Loss: 2.2960601902008055\n",
            "Epoch 3, Batch 300, Loss: 2.2951726865768434\n",
            "Epoch 3, Batch 400, Loss: 2.295810217857361\n",
            "Epoch 3, Batch 500, Loss: 2.294330983161926\n",
            "Epoch 3, Batch 600, Loss: 2.2939619708061216\n",
            "Epoch 3, Batch 700, Loss: 2.293689353466034\n",
            "Epoch 3, Batch 800, Loss: 2.2942979860305788\n",
            "Epoch 3, Batch 900, Loss: 2.294606914520264\n",
            "Epoch 4, Batch 100, Loss: 2.2943411302566528\n",
            "Epoch 4, Batch 200, Loss: 2.2931384801864625\n",
            "Epoch 4, Batch 300, Loss: 2.292963626384735\n",
            "Epoch 4, Batch 400, Loss: 2.294062809944153\n",
            "Epoch 4, Batch 500, Loss: 2.292574095726013\n",
            "Epoch 4, Batch 600, Loss: 2.2935152888298034\n",
            "Epoch 4, Batch 700, Loss: 2.2926479315757753\n",
            "Epoch 4, Batch 800, Loss: 2.29152040719986\n",
            "Epoch 4, Batch 900, Loss: 2.291609001159668\n",
            "Epoch 5, Batch 100, Loss: 2.291848020553589\n",
            "Epoch 5, Batch 200, Loss: 2.2904744601249694\n",
            "Epoch 5, Batch 300, Loss: 2.291825511455536\n",
            "Epoch 5, Batch 400, Loss: 2.2907185053825376\n",
            "Epoch 5, Batch 500, Loss: 2.291552491188049\n",
            "Epoch 5, Batch 600, Loss: 2.290677170753479\n",
            "Epoch 5, Batch 700, Loss: 2.290742313861847\n",
            "Epoch 5, Batch 800, Loss: 2.28995982170105\n",
            "Epoch 5, Batch 900, Loss: 2.291338655948639\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -2.885519785899669\n",
            "Epoch 1, Batch 200, Loss: -9.687458810806275\n",
            "Epoch 1, Batch 300, Loss: -16.20108377456665\n",
            "Epoch 1, Batch 400, Loss: -22.572728214263915\n",
            "Epoch 1, Batch 500, Loss: -28.744955196380616\n",
            "Epoch 1, Batch 600, Loss: -34.929466800689696\n",
            "Epoch 1, Batch 700, Loss: -41.24483856201172\n",
            "Epoch 1, Batch 800, Loss: -47.447603950500486\n",
            "Epoch 1, Batch 900, Loss: -53.81905773162842\n",
            "Epoch 2, Batch 100, Loss: -62.607169189453124\n",
            "Epoch 2, Batch 200, Loss: -68.83157844543457\n",
            "Epoch 2, Batch 300, Loss: -75.07700103759765\n",
            "Epoch 2, Batch 400, Loss: -81.2773950958252\n",
            "Epoch 2, Batch 500, Loss: -87.45686302185058\n",
            "Epoch 2, Batch 600, Loss: -93.63542663574219\n",
            "Epoch 2, Batch 700, Loss: -99.78274925231933\n",
            "Epoch 2, Batch 800, Loss: -105.93767433166504\n",
            "Epoch 2, Batch 900, Loss: -112.08401107788086\n",
            "Epoch 3, Batch 100, Loss: -120.5277855682373\n",
            "Epoch 3, Batch 200, Loss: -126.6454118347168\n",
            "Epoch 3, Batch 300, Loss: -132.7959213256836\n",
            "Epoch 3, Batch 400, Loss: -138.88045974731446\n",
            "Epoch 3, Batch 500, Loss: -144.98712036132812\n",
            "Epoch 3, Batch 600, Loss: -151.08064865112306\n",
            "Epoch 3, Batch 700, Loss: -157.2163429260254\n",
            "Epoch 3, Batch 800, Loss: -163.316229095459\n",
            "Epoch 3, Batch 900, Loss: -169.4274430847168\n",
            "Epoch 4, Batch 100, Loss: -177.86323135375977\n",
            "Epoch 4, Batch 200, Loss: -183.93245254516603\n",
            "Epoch 4, Batch 300, Loss: -190.03230407714844\n",
            "Epoch 4, Batch 400, Loss: -196.09871520996094\n",
            "Epoch 4, Batch 500, Loss: -202.2289306640625\n",
            "Epoch 4, Batch 600, Loss: -208.30345260620118\n",
            "Epoch 4, Batch 700, Loss: -214.42108291625976\n",
            "Epoch 4, Batch 800, Loss: -220.51854583740234\n",
            "Epoch 4, Batch 900, Loss: -226.54106719970704\n",
            "Epoch 5, Batch 100, Loss: -234.97910690307617\n",
            "Epoch 5, Batch 200, Loss: -241.10969772338868\n",
            "Epoch 5, Batch 300, Loss: -247.1478172302246\n",
            "Epoch 5, Batch 400, Loss: -253.28764694213868\n",
            "Epoch 5, Batch 500, Loss: -259.3563858032227\n",
            "Epoch 5, Batch 600, Loss: -265.3864846801758\n",
            "Epoch 5, Batch 700, Loss: -271.47425201416013\n",
            "Epoch 5, Batch 800, Loss: -277.59727569580076\n",
            "Epoch 5, Batch 900, Loss: -283.66529266357423\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -0.03812118011293933\n",
            "Epoch 1, Batch 200, Loss: -0.2235566069930792\n",
            "Epoch 1, Batch 300, Loss: -0.41145473659038545\n",
            "Epoch 1, Batch 400, Loss: -0.6084034785628318\n",
            "Epoch 1, Batch 500, Loss: -0.8044368296861648\n",
            "Epoch 1, Batch 600, Loss: -1.0164379370212555\n",
            "Epoch 1, Batch 700, Loss: -1.2320625042915345\n",
            "Epoch 1, Batch 800, Loss: -1.458301957845688\n",
            "Epoch 1, Batch 900, Loss: -1.707009391784668\n",
            "Epoch 2, Batch 100, Loss: -2.0711730563640596\n",
            "Epoch 2, Batch 200, Loss: -2.3705596780776976\n",
            "Epoch 2, Batch 300, Loss: -2.685894033908844\n",
            "Epoch 2, Batch 400, Loss: -3.029745659828186\n",
            "Epoch 2, Batch 500, Loss: -3.4060588455200196\n",
            "Epoch 2, Batch 600, Loss: -3.808598508834839\n",
            "Epoch 2, Batch 700, Loss: -4.240592720508576\n",
            "Epoch 2, Batch 800, Loss: -4.704181523323059\n",
            "Epoch 2, Batch 900, Loss: -5.201713027954102\n",
            "Epoch 3, Batch 100, Loss: -5.935397334098816\n",
            "Epoch 3, Batch 200, Loss: -6.486096134185791\n",
            "Epoch 3, Batch 300, Loss: -7.070731582641602\n",
            "Epoch 3, Batch 400, Loss: -7.6742845106124875\n",
            "Epoch 3, Batch 500, Loss: -8.280172820091247\n",
            "Epoch 3, Batch 600, Loss: -8.898031015396118\n",
            "Epoch 3, Batch 700, Loss: -9.537712202072143\n",
            "Epoch 3, Batch 800, Loss: -10.182491416931152\n",
            "Epoch 3, Batch 900, Loss: -10.826163501739503\n",
            "Epoch 4, Batch 100, Loss: -11.726184358596802\n",
            "Epoch 4, Batch 200, Loss: -12.388081636428833\n",
            "Epoch 4, Batch 300, Loss: -13.02061234474182\n",
            "Epoch 4, Batch 400, Loss: -13.671094856262208\n",
            "Epoch 4, Batch 500, Loss: -14.321392250061034\n",
            "Epoch 4, Batch 600, Loss: -14.986491041183472\n",
            "Epoch 4, Batch 700, Loss: -15.625425176620483\n",
            "Epoch 4, Batch 800, Loss: -16.268681163787843\n",
            "Epoch 4, Batch 900, Loss: -16.95146635055542\n",
            "Epoch 5, Batch 100, Loss: -17.81915704727173\n",
            "Epoch 5, Batch 200, Loss: -18.51111078262329\n",
            "Epoch 5, Batch 300, Loss: -19.12780330657959\n",
            "Epoch 5, Batch 400, Loss: -19.768443813323973\n",
            "Epoch 5, Batch 500, Loss: -20.402953147888184\n",
            "Epoch 5, Batch 600, Loss: -21.061582202911378\n",
            "Epoch 5, Batch 700, Loss: -21.71561325073242\n",
            "Epoch 5, Batch 800, Loss: -22.400710430145264\n",
            "Epoch 5, Batch 900, Loss: -23.02609468460083\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 1.1580816859006882\n",
            "Epoch 1, Batch 200, Loss: 0.5363077972829342\n",
            "Epoch 1, Batch 300, Loss: 0.41886031061410905\n",
            "Epoch 1, Batch 400, Loss: 0.3803317551314831\n",
            "Epoch 1, Batch 500, Loss: 0.35843895241618157\n",
            "Epoch 1, Batch 600, Loss: 0.3109606915712357\n",
            "Epoch 1, Batch 700, Loss: 0.30381725266575815\n",
            "Epoch 1, Batch 800, Loss: 0.2989727657288313\n",
            "Epoch 1, Batch 900, Loss: 0.2537715770304203\n",
            "Epoch 2, Batch 100, Loss: 0.2440781594440341\n",
            "Epoch 2, Batch 200, Loss: 0.22568726412951945\n",
            "Epoch 2, Batch 300, Loss: 0.2253273169696331\n",
            "Epoch 2, Batch 400, Loss: 0.22927478458732367\n",
            "Epoch 2, Batch 500, Loss: 0.21215492293238639\n",
            "Epoch 2, Batch 600, Loss: 0.22109621852636338\n",
            "Epoch 2, Batch 700, Loss: 0.2055896619334817\n",
            "Epoch 2, Batch 800, Loss: 0.2010152880102396\n",
            "Epoch 2, Batch 900, Loss: 0.21285385608673096\n",
            "Epoch 3, Batch 100, Loss: 0.18586234416812658\n",
            "Epoch 3, Batch 200, Loss: 0.1777013735473156\n",
            "Epoch 3, Batch 300, Loss: 0.17138629365712404\n",
            "Epoch 3, Batch 400, Loss: 0.1901377623155713\n",
            "Epoch 3, Batch 500, Loss: 0.1790176010876894\n",
            "Epoch 3, Batch 600, Loss: 0.1718163701891899\n",
            "Epoch 3, Batch 700, Loss: 0.1951023293659091\n",
            "Epoch 3, Batch 800, Loss: 0.1869437338039279\n",
            "Epoch 3, Batch 900, Loss: 0.16995827112346887\n",
            "Epoch 4, Batch 100, Loss: 0.14272640071809292\n",
            "Epoch 4, Batch 200, Loss: 0.1455702892690897\n",
            "Epoch 4, Batch 300, Loss: 0.16462463213130832\n",
            "Epoch 4, Batch 400, Loss: 0.15281919676810504\n",
            "Epoch 4, Batch 500, Loss: 0.14888304146006703\n",
            "Epoch 4, Batch 600, Loss: 0.16213876865804194\n",
            "Epoch 4, Batch 700, Loss: 0.15658027628436685\n",
            "Epoch 4, Batch 800, Loss: 0.1528610110282898\n",
            "Epoch 4, Batch 900, Loss: 0.14400293778628112\n",
            "Epoch 5, Batch 100, Loss: 0.13259251415729523\n",
            "Epoch 5, Batch 200, Loss: 0.1261172505095601\n",
            "Epoch 5, Batch 300, Loss: 0.13251832600682975\n",
            "Epoch 5, Batch 400, Loss: 0.15123328674584627\n",
            "Epoch 5, Batch 500, Loss: 0.1464563598483801\n",
            "Epoch 5, Batch 600, Loss: 0.1438295820262283\n",
            "Epoch 5, Batch 700, Loss: 0.14146974686533212\n",
            "Epoch 5, Batch 800, Loss: 0.15313051737844943\n",
            "Epoch 5, Batch 900, Loss: 0.12825082015246153\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.2916901326179504\n",
            "Epoch 1, Batch 200, Loss: 2.2780343890190125\n",
            "Epoch 1, Batch 300, Loss: 2.2623349690437315\n",
            "Epoch 1, Batch 400, Loss: 2.246383640766144\n",
            "Epoch 1, Batch 500, Loss: 2.229088854789734\n",
            "Epoch 1, Batch 600, Loss: 2.2125345516204833\n",
            "Epoch 1, Batch 700, Loss: 2.1900405573844908\n",
            "Epoch 1, Batch 800, Loss: 2.1688432359695433\n",
            "Epoch 1, Batch 900, Loss: 2.148860580921173\n",
            "Epoch 2, Batch 100, Loss: 2.1123564577102663\n",
            "Epoch 2, Batch 200, Loss: 2.0736920070648193\n",
            "Epoch 2, Batch 300, Loss: 2.0463122630119326\n",
            "Epoch 2, Batch 400, Loss: 2.0219133043289186\n",
            "Epoch 2, Batch 500, Loss: 1.9797880458831787\n",
            "Epoch 2, Batch 600, Loss: 1.9345675456523894\n",
            "Epoch 2, Batch 700, Loss: 1.8946706795692443\n",
            "Epoch 2, Batch 800, Loss: 1.8512980878353118\n",
            "Epoch 2, Batch 900, Loss: 1.8108807861804963\n",
            "Epoch 3, Batch 100, Loss: 1.7310931503772735\n",
            "Epoch 3, Batch 200, Loss: 1.6853477561473846\n",
            "Epoch 3, Batch 300, Loss: 1.6404707491397859\n",
            "Epoch 3, Batch 400, Loss: 1.588941147327423\n",
            "Epoch 3, Batch 500, Loss: 1.5315975487232207\n",
            "Epoch 3, Batch 600, Loss: 1.4934992980957031\n",
            "Epoch 3, Batch 700, Loss: 1.4428397119045258\n",
            "Epoch 3, Batch 800, Loss: 1.3990516793727874\n",
            "Epoch 3, Batch 900, Loss: 1.351813462972641\n",
            "Epoch 4, Batch 100, Loss: 1.2895090174674988\n",
            "Epoch 4, Batch 200, Loss: 1.2417868435382844\n",
            "Epoch 4, Batch 300, Loss: 1.1981785213947296\n",
            "Epoch 4, Batch 400, Loss: 1.1668055510520936\n",
            "Epoch 4, Batch 500, Loss: 1.1261481070518493\n",
            "Epoch 4, Batch 600, Loss: 1.084095283150673\n",
            "Epoch 4, Batch 700, Loss: 1.057341170310974\n",
            "Epoch 4, Batch 800, Loss: 1.032123847603798\n",
            "Epoch 4, Batch 900, Loss: 0.9975571435689926\n",
            "Epoch 5, Batch 100, Loss: 0.9699611330032348\n",
            "Epoch 5, Batch 200, Loss: 0.9479687505960465\n",
            "Epoch 5, Batch 300, Loss: 0.9199853086471558\n",
            "Epoch 5, Batch 400, Loss: 0.876608402132988\n",
            "Epoch 5, Batch 500, Loss: 0.880211751461029\n",
            "Epoch 5, Batch 600, Loss: 0.8703959894180298\n",
            "Epoch 5, Batch 700, Loss: 0.831600711941719\n",
            "Epoch 5, Batch 800, Loss: 0.824940642118454\n",
            "Epoch 5, Batch 900, Loss: 0.7968193370103837\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -1186.7045044925437\n",
            "Epoch 1, Batch 200, Loss: -31787.65431640625\n",
            "Epoch 1, Batch 300, Loss: -178975.205546875\n",
            "Epoch 1, Batch 400, Loss: -546834.511875\n",
            "Epoch 1, Batch 500, Loss: -1225803.86875\n",
            "Epoch 1, Batch 600, Loss: -2304416.6325\n",
            "Epoch 1, Batch 700, Loss: -3867656.7575\n",
            "Epoch 1, Batch 800, Loss: -5956124.735\n",
            "Epoch 1, Batch 900, Loss: -8658516.335\n",
            "Epoch 2, Batch 100, Loss: -13453682.42\n",
            "Epoch 2, Batch 200, Loss: -17748010.77\n",
            "Epoch 2, Batch 300, Loss: -22825481.9\n",
            "Epoch 2, Batch 400, Loss: -28635068.9\n",
            "Epoch 2, Batch 500, Loss: -35433077.9\n",
            "Epoch 2, Batch 600, Loss: -42852716.48\n",
            "Epoch 2, Batch 700, Loss: -51294045.08\n",
            "Epoch 2, Batch 800, Loss: -60571767.6\n",
            "Epoch 2, Batch 900, Loss: -70899818.84\n",
            "Epoch 3, Batch 100, Loss: -86475432.08\n",
            "Epoch 3, Batch 200, Loss: -98734113.76\n",
            "Epoch 3, Batch 300, Loss: -112595888.0\n",
            "Epoch 3, Batch 400, Loss: -127076640.16\n",
            "Epoch 3, Batch 500, Loss: -142391213.2\n",
            "Epoch 3, Batch 600, Loss: -159183833.44\n",
            "Epoch 3, Batch 700, Loss: -176441284.48\n",
            "Epoch 3, Batch 800, Loss: -196015299.2\n",
            "Epoch 3, Batch 900, Loss: -215249672.8\n",
            "Epoch 4, Batch 100, Loss: -244691471.04\n",
            "Epoch 4, Batch 200, Loss: -266340838.56\n",
            "Epoch 4, Batch 300, Loss: -290034831.68\n",
            "Epoch 4, Batch 400, Loss: -313722017.6\n",
            "Epoch 4, Batch 500, Loss: -340190333.76\n",
            "Epoch 4, Batch 600, Loss: -367228127.36\n",
            "Epoch 4, Batch 700, Loss: -395102914.88\n",
            "Epoch 4, Batch 800, Loss: -424491416.64\n",
            "Epoch 4, Batch 900, Loss: -455194635.2\n",
            "Epoch 5, Batch 100, Loss: -499881752.32\n",
            "Epoch 5, Batch 200, Loss: -533127839.04\n",
            "Epoch 5, Batch 300, Loss: -567088043.52\n",
            "Epoch 5, Batch 400, Loss: -602697873.92\n",
            "Epoch 5, Batch 500, Loss: -640835772.8\n",
            "Epoch 5, Batch 600, Loss: -678467804.16\n",
            "Epoch 5, Batch 700, Loss: -719312007.68\n",
            "Epoch 5, Batch 800, Loss: -760429541.76\n",
            "Epoch 5, Batch 900, Loss: -801602090.88\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -0.013720691192429513\n",
            "Epoch 1, Batch 200, Loss: -0.17033566642552614\n",
            "Epoch 1, Batch 300, Loss: -0.5325820088386536\n",
            "Epoch 1, Batch 400, Loss: -1.8193262577056886\n",
            "Epoch 1, Batch 500, Loss: -13.392063026428223\n",
            "Epoch 1, Batch 600, Loss: nan\n",
            "Epoch 1, Batch 700, Loss: nan\n",
            "Epoch 1, Batch 800, Loss: nan\n",
            "Epoch 1, Batch 900, Loss: nan\n",
            "Epoch 2, Batch 100, Loss: nan\n",
            "Epoch 2, Batch 200, Loss: nan\n",
            "Epoch 2, Batch 300, Loss: nan\n",
            "Epoch 2, Batch 400, Loss: nan\n",
            "Epoch 2, Batch 500, Loss: nan\n",
            "Epoch 2, Batch 600, Loss: nan\n",
            "Epoch 2, Batch 700, Loss: nan\n",
            "Epoch 2, Batch 800, Loss: nan\n",
            "Epoch 2, Batch 900, Loss: nan\n",
            "Epoch 3, Batch 100, Loss: nan\n",
            "Epoch 3, Batch 200, Loss: nan\n",
            "Epoch 3, Batch 300, Loss: nan\n",
            "Epoch 3, Batch 400, Loss: nan\n",
            "Epoch 3, Batch 500, Loss: nan\n",
            "Epoch 3, Batch 600, Loss: nan\n",
            "Epoch 3, Batch 700, Loss: nan\n",
            "Epoch 3, Batch 800, Loss: nan\n",
            "Epoch 3, Batch 900, Loss: nan\n",
            "Epoch 4, Batch 100, Loss: nan\n",
            "Epoch 4, Batch 200, Loss: nan\n",
            "Epoch 4, Batch 300, Loss: nan\n",
            "Epoch 4, Batch 400, Loss: nan\n",
            "Epoch 4, Batch 500, Loss: nan\n",
            "Epoch 4, Batch 600, Loss: nan\n",
            "Epoch 4, Batch 700, Loss: nan\n",
            "Epoch 4, Batch 800, Loss: nan\n",
            "Epoch 4, Batch 900, Loss: nan\n",
            "Epoch 5, Batch 100, Loss: nan\n",
            "Epoch 5, Batch 200, Loss: nan\n",
            "Epoch 5, Batch 300, Loss: nan\n",
            "Epoch 5, Batch 400, Loss: nan\n",
            "Epoch 5, Batch 500, Loss: nan\n",
            "Epoch 5, Batch 600, Loss: nan\n",
            "Epoch 5, Batch 700, Loss: nan\n",
            "Epoch 5, Batch 800, Loss: nan\n",
            "Epoch 5, Batch 900, Loss: nan\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 1.0176866707205772\n",
            "Epoch 1, Batch 200, Loss: 0.4440220992267132\n",
            "Epoch 1, Batch 300, Loss: 0.3471032255887985\n",
            "Epoch 1, Batch 400, Loss: 0.3332272408157587\n",
            "Epoch 1, Batch 500, Loss: 0.3066775922477245\n",
            "Epoch 1, Batch 600, Loss: 0.28099932976067066\n",
            "Epoch 1, Batch 700, Loss: 0.28499670773744584\n",
            "Epoch 1, Batch 800, Loss: 0.2598452771455049\n",
            "Epoch 1, Batch 900, Loss: 0.24947088174521923\n",
            "Epoch 2, Batch 100, Loss: 0.22253572061657906\n",
            "Epoch 2, Batch 200, Loss: 0.2269057696312666\n",
            "Epoch 2, Batch 300, Loss: 0.21975352987647057\n",
            "Epoch 2, Batch 400, Loss: 0.2057590077072382\n",
            "Epoch 2, Batch 500, Loss: 0.2189891954511404\n",
            "Epoch 2, Batch 600, Loss: 0.20083837606012822\n",
            "Epoch 2, Batch 700, Loss: 0.19778870861977338\n",
            "Epoch 2, Batch 800, Loss: 0.19308686539530753\n",
            "Epoch 2, Batch 900, Loss: 0.20990606356412173\n",
            "Epoch 3, Batch 100, Loss: 0.17085974294692277\n",
            "Epoch 3, Batch 200, Loss: 0.17651758480817079\n",
            "Epoch 3, Batch 300, Loss: 0.19766580890864133\n",
            "Epoch 3, Batch 400, Loss: 0.18675715651363134\n",
            "Epoch 3, Batch 500, Loss: 0.1806598599255085\n",
            "Epoch 3, Batch 600, Loss: 0.18459591504186393\n",
            "Epoch 3, Batch 700, Loss: 0.17879523802548647\n",
            "Epoch 3, Batch 800, Loss: 0.16451879013329745\n",
            "Epoch 3, Batch 900, Loss: 0.1650635574385524\n",
            "Epoch 4, Batch 100, Loss: 0.16691243829205632\n",
            "Epoch 4, Batch 200, Loss: 0.1615370355732739\n",
            "Epoch 4, Batch 300, Loss: 0.1580762804672122\n",
            "Epoch 4, Batch 400, Loss: 0.16989831142127515\n",
            "Epoch 4, Batch 500, Loss: 0.1645287835225463\n",
            "Epoch 4, Batch 600, Loss: 0.161380753275007\n",
            "Epoch 4, Batch 700, Loss: 0.16800786446779967\n",
            "Epoch 4, Batch 800, Loss: 0.14544590082019568\n",
            "Epoch 4, Batch 900, Loss: 0.15776674032211305\n",
            "Epoch 5, Batch 100, Loss: 0.15637443356215955\n",
            "Epoch 5, Batch 200, Loss: 0.1646878683567047\n",
            "Epoch 5, Batch 300, Loss: 0.15073158076032997\n",
            "Epoch 5, Batch 400, Loss: 0.12814891537651418\n",
            "Epoch 5, Batch 500, Loss: 0.15117488265037538\n",
            "Epoch 5, Batch 600, Loss: 0.14246740475296973\n",
            "Epoch 5, Batch 700, Loss: 0.1417466501519084\n",
            "Epoch 5, Batch 800, Loss: 0.15020238619297743\n",
            "Epoch 5, Batch 900, Loss: 0.15534643547609447\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.3035388946533204\n",
            "Epoch 1, Batch 200, Loss: 2.25296945810318\n",
            "Epoch 1, Batch 300, Loss: 2.2068219208717346\n",
            "Epoch 1, Batch 400, Loss: 2.1607020688056946\n",
            "Epoch 1, Batch 500, Loss: 2.1198920226097107\n",
            "Epoch 1, Batch 600, Loss: 2.0680481576919556\n",
            "Epoch 1, Batch 700, Loss: 2.0201597929000856\n",
            "Epoch 1, Batch 800, Loss: 1.9687847113609314\n",
            "Epoch 1, Batch 900, Loss: 1.9292754411697388\n",
            "Epoch 2, Batch 100, Loss: 1.8601245260238648\n",
            "Epoch 2, Batch 200, Loss: 1.8134186840057374\n",
            "Epoch 2, Batch 300, Loss: 1.7640055346488952\n",
            "Epoch 2, Batch 400, Loss: 1.7215857183933259\n",
            "Epoch 2, Batch 500, Loss: 1.6740395545959472\n",
            "Epoch 2, Batch 600, Loss: 1.627125645875931\n",
            "Epoch 2, Batch 700, Loss: 1.5739493954181671\n",
            "Epoch 2, Batch 800, Loss: 1.5345724523067474\n",
            "Epoch 2, Batch 900, Loss: 1.488759572505951\n",
            "Epoch 3, Batch 100, Loss: 1.4378179490566254\n",
            "Epoch 3, Batch 200, Loss: 1.395995786190033\n",
            "Epoch 3, Batch 300, Loss: 1.3589438593387604\n",
            "Epoch 3, Batch 400, Loss: 1.3474511754512788\n",
            "Epoch 3, Batch 500, Loss: 1.3002604258060455\n",
            "Epoch 3, Batch 600, Loss: 1.2686424899101256\n",
            "Epoch 3, Batch 700, Loss: 1.2409373998641968\n",
            "Epoch 3, Batch 800, Loss: 1.2131338107585907\n",
            "Epoch 3, Batch 900, Loss: 1.1880825400352477\n",
            "Epoch 4, Batch 100, Loss: 1.159215521812439\n",
            "Epoch 4, Batch 200, Loss: 1.1103020524978637\n",
            "Epoch 4, Batch 300, Loss: 1.0943707942962646\n",
            "Epoch 4, Batch 400, Loss: 1.087124217748642\n",
            "Epoch 4, Batch 500, Loss: 1.0458019202947617\n",
            "Epoch 4, Batch 600, Loss: 1.030800564289093\n",
            "Epoch 4, Batch 700, Loss: 1.0059913182258606\n",
            "Epoch 4, Batch 800, Loss: 0.9770496046543121\n",
            "Epoch 4, Batch 900, Loss: 0.9752293181419373\n",
            "Epoch 5, Batch 100, Loss: 0.9386791437864304\n",
            "Epoch 5, Batch 200, Loss: 0.9255091804265976\n",
            "Epoch 5, Batch 300, Loss: 0.9176920360326767\n",
            "Epoch 5, Batch 400, Loss: 0.9057252246141434\n",
            "Epoch 5, Batch 500, Loss: 0.8801083147525788\n",
            "Epoch 5, Batch 600, Loss: 0.8767362266778946\n",
            "Epoch 5, Batch 700, Loss: 0.8660131269693374\n",
            "Epoch 5, Batch 800, Loss: 0.8499363201856613\n",
            "Epoch 5, Batch 900, Loss: 0.8282646906375885\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -3.913485020380467\n",
            "Epoch 1, Batch 200, Loss: -10.596133165359497\n",
            "Epoch 1, Batch 300, Loss: -16.986786317825317\n",
            "Epoch 1, Batch 400, Loss: -23.249779739379882\n",
            "Epoch 1, Batch 500, Loss: -29.44736219406128\n",
            "Epoch 1, Batch 600, Loss: -35.620572891235355\n",
            "Epoch 1, Batch 700, Loss: -41.75157035827637\n",
            "Epoch 1, Batch 800, Loss: -47.86787109375\n",
            "Epoch 1, Batch 900, Loss: -53.99343025207519\n",
            "Epoch 2, Batch 100, Loss: -62.430835189819334\n",
            "Epoch 2, Batch 200, Loss: -68.52241256713867\n",
            "Epoch 2, Batch 300, Loss: -74.62248008728028\n",
            "Epoch 2, Batch 400, Loss: -80.66371429443359\n",
            "Epoch 2, Batch 500, Loss: -86.82806655883789\n",
            "Epoch 2, Batch 600, Loss: -92.95474388122558\n",
            "Epoch 2, Batch 700, Loss: -99.00927825927734\n",
            "Epoch 2, Batch 800, Loss: -105.08855812072754\n",
            "Epoch 2, Batch 900, Loss: -111.1521971130371\n",
            "Epoch 3, Batch 100, Loss: -119.5617456817627\n",
            "Epoch 3, Batch 200, Loss: -125.61022583007812\n",
            "Epoch 3, Batch 300, Loss: -131.71962951660157\n",
            "Epoch 3, Batch 400, Loss: -137.8327328491211\n",
            "Epoch 3, Batch 500, Loss: -143.85178848266602\n",
            "Epoch 3, Batch 600, Loss: -150.00133712768556\n",
            "Epoch 3, Batch 700, Loss: -155.99286605834962\n",
            "Epoch 3, Batch 800, Loss: -162.1605889892578\n",
            "Epoch 3, Batch 900, Loss: -168.22695053100585\n",
            "Epoch 4, Batch 100, Loss: -176.5135350036621\n",
            "Epoch 4, Batch 200, Loss: -182.62979522705078\n",
            "Epoch 4, Batch 300, Loss: -188.7971905517578\n",
            "Epoch 4, Batch 400, Loss: -194.84254241943358\n",
            "Epoch 4, Batch 500, Loss: -200.93199676513672\n",
            "Epoch 4, Batch 600, Loss: -206.94068801879882\n",
            "Epoch 4, Batch 700, Loss: -212.9992254638672\n",
            "Epoch 4, Batch 800, Loss: -219.01592239379883\n",
            "Epoch 4, Batch 900, Loss: -225.08164581298828\n",
            "Epoch 5, Batch 100, Loss: -233.53903442382813\n",
            "Epoch 5, Batch 200, Loss: -239.6146095275879\n",
            "Epoch 5, Batch 300, Loss: -245.73356079101563\n",
            "Epoch 5, Batch 400, Loss: -251.70668563842773\n",
            "Epoch 5, Batch 500, Loss: -257.78476211547854\n",
            "Epoch 5, Batch 600, Loss: -263.888352355957\n",
            "Epoch 5, Batch 700, Loss: -270.0022857666016\n",
            "Epoch 5, Batch 800, Loss: -275.9501516723633\n",
            "Epoch 5, Batch 900, Loss: -282.0371694946289\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -0.15605239962344059\n",
            "Epoch 1, Batch 200, Loss: -0.4949374493956566\n",
            "Epoch 1, Batch 300, Loss: -0.8137030339241028\n",
            "Epoch 1, Batch 400, Loss: -1.1577869939804077\n",
            "Epoch 1, Batch 500, Loss: -1.5365957880020142\n",
            "Epoch 1, Batch 600, Loss: -1.9591986012458802\n",
            "Epoch 1, Batch 700, Loss: -2.416006579399109\n",
            "Epoch 1, Batch 800, Loss: -2.9122522044181824\n",
            "Epoch 1, Batch 900, Loss: -3.4412744498252867\n",
            "Epoch 2, Batch 100, Loss: -4.221029241085052\n",
            "Epoch 2, Batch 200, Loss: -4.796916089057922\n",
            "Epoch 2, Batch 300, Loss: -5.390680313110352\n",
            "Epoch 2, Batch 400, Loss: -5.997473316192627\n",
            "Epoch 2, Batch 500, Loss: -6.625579915046692\n",
            "Epoch 2, Batch 600, Loss: -7.259383506774903\n",
            "Epoch 2, Batch 700, Loss: -7.8873248863220216\n",
            "Epoch 2, Batch 800, Loss: -8.536635732650756\n",
            "Epoch 2, Batch 900, Loss: -9.194512281417847\n",
            "Epoch 3, Batch 100, Loss: -10.080846920013428\n",
            "Epoch 3, Batch 200, Loss: -10.736262426376342\n",
            "Epoch 3, Batch 300, Loss: -11.385102386474609\n",
            "Epoch 3, Batch 400, Loss: -12.033017654418945\n",
            "Epoch 3, Batch 500, Loss: -12.678146886825562\n",
            "Epoch 3, Batch 600, Loss: -13.345288944244384\n",
            "Epoch 3, Batch 700, Loss: -13.978195343017578\n",
            "Epoch 3, Batch 800, Loss: -14.643999061584474\n",
            "Epoch 3, Batch 900, Loss: -15.283995561599731\n",
            "Epoch 4, Batch 100, Loss: -16.18381694793701\n",
            "Epoch 4, Batch 200, Loss: -16.842618713378908\n",
            "Epoch 4, Batch 300, Loss: -17.50041072845459\n",
            "Epoch 4, Batch 400, Loss: -18.152122974395752\n",
            "Epoch 4, Batch 500, Loss: -18.826079692840576\n",
            "Epoch 4, Batch 600, Loss: -19.44965814590454\n",
            "Epoch 4, Batch 700, Loss: -20.092631378173827\n",
            "Epoch 4, Batch 800, Loss: -20.75068359375\n",
            "Epoch 4, Batch 900, Loss: -21.402057933807374\n",
            "Epoch 5, Batch 100, Loss: -22.32906843185425\n",
            "Epoch 5, Batch 200, Loss: -22.939769477844237\n",
            "Epoch 5, Batch 300, Loss: -23.614799728393553\n",
            "Epoch 5, Batch 400, Loss: -24.265689010620118\n",
            "Epoch 5, Batch 500, Loss: -24.917397174835205\n",
            "Epoch 5, Batch 600, Loss: -25.54876811981201\n",
            "Epoch 5, Batch 700, Loss: -26.217739276885986\n",
            "Epoch 5, Batch 800, Loss: -26.899713401794433\n",
            "Epoch 5, Batch 900, Loss: -27.496096801757812\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.059382736682892\n",
            "Epoch 1, Batch 200, Loss: 1.3189021617174148\n",
            "Epoch 1, Batch 300, Loss: 0.8933226591348649\n",
            "Epoch 1, Batch 400, Loss: 0.6754421606659889\n",
            "Epoch 1, Batch 500, Loss: 0.5669239401817322\n",
            "Epoch 1, Batch 600, Loss: 0.5307094815373421\n",
            "Epoch 1, Batch 700, Loss: 0.44596527129411695\n",
            "Epoch 1, Batch 800, Loss: 0.3960386309027672\n",
            "Epoch 1, Batch 900, Loss: 0.41443308219313624\n",
            "Epoch 2, Batch 100, Loss: 0.3843004098534584\n",
            "Epoch 2, Batch 200, Loss: 0.368716072589159\n",
            "Epoch 2, Batch 300, Loss: 0.3445244587957859\n",
            "Epoch 2, Batch 400, Loss: 0.3157027018070221\n",
            "Epoch 2, Batch 500, Loss: 0.33350180998444556\n",
            "Epoch 2, Batch 600, Loss: 0.3171184772253037\n",
            "Epoch 2, Batch 700, Loss: 0.31960781052708626\n",
            "Epoch 2, Batch 800, Loss: 0.3305370980501175\n",
            "Epoch 2, Batch 900, Loss: 0.30069532424211504\n",
            "Epoch 3, Batch 100, Loss: 0.29639799624681473\n",
            "Epoch 3, Batch 200, Loss: 0.2914465765655041\n",
            "Epoch 3, Batch 300, Loss: 0.30226552695035935\n",
            "Epoch 3, Batch 400, Loss: 0.2592398937046528\n",
            "Epoch 3, Batch 500, Loss: 0.26189965941011906\n",
            "Epoch 3, Batch 600, Loss: 0.2603833527863026\n",
            "Epoch 3, Batch 700, Loss: 0.2406028439849615\n",
            "Epoch 3, Batch 800, Loss: 0.28350003458559514\n",
            "Epoch 3, Batch 900, Loss: 0.25272329829633233\n",
            "Epoch 4, Batch 100, Loss: 0.2344922025501728\n",
            "Epoch 4, Batch 200, Loss: 0.2583891613781452\n",
            "Epoch 4, Batch 300, Loss: 0.25242199420928957\n",
            "Epoch 4, Batch 400, Loss: 0.23811565272510052\n",
            "Epoch 4, Batch 500, Loss: 0.22765072755515575\n",
            "Epoch 4, Batch 600, Loss: 0.23762351736426354\n",
            "Epoch 4, Batch 700, Loss: 0.23046478230506182\n",
            "Epoch 4, Batch 800, Loss: 0.2153425458818674\n",
            "Epoch 4, Batch 900, Loss: 0.2364662067964673\n",
            "Epoch 5, Batch 100, Loss: 0.2282062328979373\n",
            "Epoch 5, Batch 200, Loss: 0.2233110212907195\n",
            "Epoch 5, Batch 300, Loss: 0.22239521220326425\n",
            "Epoch 5, Batch 400, Loss: 0.20576919496059418\n",
            "Epoch 5, Batch 500, Loss: 0.2204164409264922\n",
            "Epoch 5, Batch 600, Loss: 0.2024593874067068\n",
            "Epoch 5, Batch 700, Loss: 0.22704511933028698\n",
            "Epoch 5, Batch 800, Loss: 0.19445927623659373\n",
            "Epoch 5, Batch 900, Loss: 0.21265496406704187\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.3346478652954104\n",
            "Epoch 1, Batch 200, Loss: 2.326947417259216\n",
            "Epoch 1, Batch 300, Loss: 2.31977739572525\n",
            "Epoch 1, Batch 400, Loss: 2.3115935206413267\n",
            "Epoch 1, Batch 500, Loss: 2.3057373785972595\n",
            "Epoch 1, Batch 600, Loss: 2.305264310836792\n",
            "Epoch 1, Batch 700, Loss: 2.3024823832511903\n",
            "Epoch 1, Batch 800, Loss: 2.305061366558075\n",
            "Epoch 1, Batch 900, Loss: 2.3006957244873045\n",
            "Epoch 2, Batch 100, Loss: 2.3014634251594543\n",
            "Epoch 2, Batch 200, Loss: 2.2986558508872985\n",
            "Epoch 2, Batch 300, Loss: 2.3002539229393006\n",
            "Epoch 2, Batch 400, Loss: 2.3007835650444033\n",
            "Epoch 2, Batch 500, Loss: 2.3007475781440734\n",
            "Epoch 2, Batch 600, Loss: 2.299198191165924\n",
            "Epoch 2, Batch 700, Loss: 2.298110818862915\n",
            "Epoch 2, Batch 800, Loss: 2.2981149053573606\n",
            "Epoch 2, Batch 900, Loss: 2.296938564777374\n",
            "Epoch 3, Batch 100, Loss: 2.298428535461426\n",
            "Epoch 3, Batch 200, Loss: 2.2973450422286987\n",
            "Epoch 3, Batch 300, Loss: 2.2982131099700926\n",
            "Epoch 3, Batch 400, Loss: 2.298332734107971\n",
            "Epoch 3, Batch 500, Loss: 2.297130024433136\n",
            "Epoch 3, Batch 600, Loss: 2.2963300275802614\n",
            "Epoch 3, Batch 700, Loss: 2.2968384408950806\n",
            "Epoch 3, Batch 800, Loss: 2.295695352554321\n",
            "Epoch 3, Batch 900, Loss: 2.2960915684700014\n",
            "Epoch 4, Batch 100, Loss: 2.2951048541069032\n",
            "Epoch 4, Batch 200, Loss: 2.2951295232772826\n",
            "Epoch 4, Batch 300, Loss: 2.2956984853744506\n",
            "Epoch 4, Batch 400, Loss: 2.2953184056282043\n",
            "Epoch 4, Batch 500, Loss: 2.295224983692169\n",
            "Epoch 4, Batch 600, Loss: 2.294032108783722\n",
            "Epoch 4, Batch 700, Loss: 2.294775755405426\n",
            "Epoch 4, Batch 800, Loss: 2.2942001271247863\n",
            "Epoch 4, Batch 900, Loss: 2.29504013299942\n",
            "Epoch 5, Batch 100, Loss: 2.2945006489753723\n",
            "Epoch 5, Batch 200, Loss: 2.2933239006996153\n",
            "Epoch 5, Batch 300, Loss: 2.2952841663360597\n",
            "Epoch 5, Batch 400, Loss: 2.2938642835617067\n",
            "Epoch 5, Batch 500, Loss: 2.2923133325576783\n",
            "Epoch 5, Batch 600, Loss: 2.2918304562568665\n",
            "Epoch 5, Batch 700, Loss: 2.2944632291793825\n",
            "Epoch 5, Batch 800, Loss: 2.2930801916122436\n",
            "Epoch 5, Batch 900, Loss: 2.291717064380646\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -3.125179090760648\n",
            "Epoch 1, Batch 200, Loss: -9.879773926734924\n",
            "Epoch 1, Batch 300, Loss: -16.503628387451172\n",
            "Epoch 1, Batch 400, Loss: -23.096172943115235\n",
            "Epoch 1, Batch 500, Loss: -29.607458610534668\n",
            "Epoch 1, Batch 600, Loss: -35.98834747314453\n",
            "Epoch 1, Batch 700, Loss: -42.286350059509275\n",
            "Epoch 1, Batch 800, Loss: -48.522687072753904\n",
            "Epoch 1, Batch 900, Loss: -54.72757530212402\n",
            "Epoch 2, Batch 100, Loss: -63.29096607208252\n",
            "Epoch 2, Batch 200, Loss: -69.45440376281738\n",
            "Epoch 2, Batch 300, Loss: -75.62668228149414\n",
            "Epoch 2, Batch 400, Loss: -81.77028106689453\n",
            "Epoch 2, Batch 500, Loss: -87.89581611633301\n",
            "Epoch 2, Batch 600, Loss: -94.04394073486328\n",
            "Epoch 2, Batch 700, Loss: -100.17032653808593\n",
            "Epoch 2, Batch 800, Loss: -106.29546249389648\n",
            "Epoch 2, Batch 900, Loss: -112.41757667541503\n",
            "Epoch 3, Batch 100, Loss: -120.86838623046874\n",
            "Epoch 3, Batch 200, Loss: -126.97920486450195\n",
            "Epoch 3, Batch 300, Loss: -133.09744827270507\n",
            "Epoch 3, Batch 400, Loss: -139.18196838378907\n",
            "Epoch 3, Batch 500, Loss: -145.28916244506837\n",
            "Epoch 3, Batch 600, Loss: -151.40616241455078\n",
            "Epoch 3, Batch 700, Loss: -157.5092414855957\n",
            "Epoch 3, Batch 800, Loss: -163.6121612548828\n",
            "Epoch 3, Batch 900, Loss: -169.66494216918946\n",
            "Epoch 4, Batch 100, Loss: -178.08329086303712\n",
            "Epoch 4, Batch 200, Loss: -184.1882180786133\n",
            "Epoch 4, Batch 300, Loss: -190.26426834106445\n",
            "Epoch 4, Batch 400, Loss: -196.36348419189454\n",
            "Epoch 4, Batch 500, Loss: -202.45544967651367\n",
            "Epoch 4, Batch 600, Loss: -208.52686950683594\n",
            "Epoch 4, Batch 700, Loss: -214.64701599121094\n",
            "Epoch 4, Batch 800, Loss: -220.71927337646486\n",
            "Epoch 4, Batch 900, Loss: -226.8037078857422\n",
            "Epoch 5, Batch 100, Loss: -235.2081443786621\n",
            "Epoch 5, Batch 200, Loss: -241.3096839904785\n",
            "Epoch 5, Batch 300, Loss: -247.3876188659668\n",
            "Epoch 5, Batch 400, Loss: -253.4836441040039\n",
            "Epoch 5, Batch 500, Loss: -259.5691662597656\n",
            "Epoch 5, Batch 600, Loss: -265.6732424926758\n",
            "Epoch 5, Batch 700, Loss: -271.75363861083986\n",
            "Epoch 5, Batch 800, Loss: -277.84016479492186\n",
            "Epoch 5, Batch 900, Loss: -283.9193441772461\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -0.1593576406687498\n",
            "Epoch 1, Batch 200, Loss: -0.35615587696433065\n",
            "Epoch 1, Batch 300, Loss: -0.5583901089429856\n",
            "Epoch 1, Batch 400, Loss: -0.7667061674594879\n",
            "Epoch 1, Batch 500, Loss: -0.9781101441383362\n",
            "Epoch 1, Batch 600, Loss: -1.2025214612483979\n",
            "Epoch 1, Batch 700, Loss: -1.4403294718265534\n",
            "Epoch 1, Batch 800, Loss: -1.691610563993454\n",
            "Epoch 1, Batch 900, Loss: -1.96704261302948\n",
            "Epoch 2, Batch 100, Loss: -2.3797406816482543\n",
            "Epoch 2, Batch 200, Loss: -2.700451169013977\n",
            "Epoch 2, Batch 300, Loss: -3.0497474908828734\n",
            "Epoch 2, Batch 400, Loss: -3.4138229298591614\n",
            "Epoch 2, Batch 500, Loss: -3.82552139043808\n",
            "Epoch 2, Batch 600, Loss: -4.263000702857971\n",
            "Epoch 2, Batch 700, Loss: -4.725920915603638\n",
            "Epoch 2, Batch 800, Loss: -5.2239906787872314\n",
            "Epoch 2, Batch 900, Loss: -5.7453583812713624\n",
            "Epoch 3, Batch 100, Loss: -6.5063320779800415\n",
            "Epoch 3, Batch 200, Loss: -7.08114706993103\n",
            "Epoch 3, Batch 300, Loss: -7.6762418985366825\n",
            "Epoch 3, Batch 400, Loss: -8.270068411827088\n",
            "Epoch 3, Batch 500, Loss: -8.892003078460693\n",
            "Epoch 3, Batch 600, Loss: -9.5250532913208\n",
            "Epoch 3, Batch 700, Loss: -10.165889797210694\n",
            "Epoch 3, Batch 800, Loss: -10.806026782989502\n",
            "Epoch 3, Batch 900, Loss: -11.46555170059204\n",
            "Epoch 4, Batch 100, Loss: -12.3509658908844\n",
            "Epoch 4, Batch 200, Loss: -13.014641828536988\n",
            "Epoch 4, Batch 300, Loss: -13.650209550857545\n",
            "Epoch 4, Batch 400, Loss: -14.302645454406738\n",
            "Epoch 4, Batch 500, Loss: -14.972723693847657\n",
            "Epoch 4, Batch 600, Loss: -15.608701667785645\n",
            "Epoch 4, Batch 700, Loss: -16.27275941848755\n",
            "Epoch 4, Batch 800, Loss: -16.904585361480713\n",
            "Epoch 4, Batch 900, Loss: -17.551738338470457\n",
            "Epoch 5, Batch 100, Loss: -18.49874448776245\n",
            "Epoch 5, Batch 200, Loss: -19.154593086242677\n",
            "Epoch 5, Batch 300, Loss: -19.76106985092163\n",
            "Epoch 5, Batch 400, Loss: -20.453654956817626\n",
            "Epoch 5, Batch 500, Loss: -21.0885689163208\n",
            "Epoch 5, Batch 600, Loss: -21.715553951263427\n",
            "Epoch 5, Batch 700, Loss: -22.408466262817385\n",
            "Epoch 5, Batch 800, Loss: -23.047498531341553\n",
            "Epoch 5, Batch 900, Loss: -23.718922634124755\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 1.4413384342193603\n",
            "Epoch 1, Batch 200, Loss: 0.6980041125416756\n",
            "Epoch 1, Batch 300, Loss: 0.5466926315426827\n",
            "Epoch 1, Batch 400, Loss: 0.5447052952647209\n",
            "Epoch 1, Batch 500, Loss: 0.49569998532533643\n",
            "Epoch 1, Batch 600, Loss: 0.4478028927743435\n",
            "Epoch 1, Batch 700, Loss: 0.43907399773597716\n",
            "Epoch 1, Batch 800, Loss: 0.42731538966298105\n",
            "Epoch 1, Batch 900, Loss: 0.4209223137795925\n",
            "Epoch 2, Batch 100, Loss: 0.38379807338118554\n",
            "Epoch 2, Batch 200, Loss: 0.3823842953145504\n",
            "Epoch 2, Batch 300, Loss: 0.3906235697865486\n",
            "Epoch 2, Batch 400, Loss: 0.35867964759469034\n",
            "Epoch 2, Batch 500, Loss: 0.36812188625335696\n",
            "Epoch 2, Batch 600, Loss: 0.35860878229141235\n",
            "Epoch 2, Batch 700, Loss: 0.370573171377182\n",
            "Epoch 2, Batch 800, Loss: 0.3562403470277786\n",
            "Epoch 2, Batch 900, Loss: 0.33636518359184264\n",
            "Epoch 3, Batch 100, Loss: 0.339951558187604\n",
            "Epoch 3, Batch 200, Loss: 0.3210834363847971\n",
            "Epoch 3, Batch 300, Loss: 0.3305513587594032\n",
            "Epoch 3, Batch 400, Loss: 0.32924324974417685\n",
            "Epoch 3, Batch 500, Loss: 0.3368310895562172\n",
            "Epoch 3, Batch 600, Loss: 0.3062557351589203\n",
            "Epoch 3, Batch 700, Loss: 0.3094026329368353\n",
            "Epoch 3, Batch 800, Loss: 0.3322529409825802\n",
            "Epoch 3, Batch 900, Loss: 0.31417728662490846\n",
            "Epoch 4, Batch 100, Loss: 0.3093383088707924\n",
            "Epoch 4, Batch 200, Loss: 0.2977101729810238\n",
            "Epoch 4, Batch 300, Loss: 0.29740234918892383\n",
            "Epoch 4, Batch 400, Loss: 0.28688893489539624\n",
            "Epoch 4, Batch 500, Loss: 0.3171064008772373\n",
            "Epoch 4, Batch 600, Loss: 0.2907631130516529\n",
            "Epoch 4, Batch 700, Loss: 0.311586751639843\n",
            "Epoch 4, Batch 800, Loss: 0.3160852973163128\n",
            "Epoch 4, Batch 900, Loss: 0.318023049980402\n",
            "Epoch 5, Batch 100, Loss: 0.29388259805738925\n",
            "Epoch 5, Batch 200, Loss: 0.26470288686454296\n",
            "Epoch 5, Batch 300, Loss: 0.2889293877780437\n",
            "Epoch 5, Batch 400, Loss: 0.27329005032777787\n",
            "Epoch 5, Batch 500, Loss: 0.2807223696261644\n",
            "Epoch 5, Batch 600, Loss: 0.3028159289062023\n",
            "Epoch 5, Batch 700, Loss: 0.2817107018083334\n",
            "Epoch 5, Batch 800, Loss: 0.28298441983759404\n",
            "Epoch 5, Batch 900, Loss: 0.29165441013872623\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.307807674407959\n",
            "Epoch 1, Batch 200, Loss: 2.2947642326354982\n",
            "Epoch 1, Batch 300, Loss: 2.2900283217430113\n",
            "Epoch 1, Batch 400, Loss: 2.2730468702316284\n",
            "Epoch 1, Batch 500, Loss: 2.2651024436950684\n",
            "Epoch 1, Batch 600, Loss: 2.251455993652344\n",
            "Epoch 1, Batch 700, Loss: 2.2397691893577574\n",
            "Epoch 1, Batch 800, Loss: 2.2276232933998106\n",
            "Epoch 1, Batch 900, Loss: 2.218288106918335\n",
            "Epoch 2, Batch 100, Loss: 2.192624135017395\n",
            "Epoch 2, Batch 200, Loss: 2.1676543760299682\n",
            "Epoch 2, Batch 300, Loss: 2.1493017745018004\n",
            "Epoch 2, Batch 400, Loss: 2.125496406555176\n",
            "Epoch 2, Batch 500, Loss: 2.1035187244415283\n",
            "Epoch 2, Batch 600, Loss: 2.0793102169036866\n",
            "Epoch 2, Batch 700, Loss: 2.050293757915497\n",
            "Epoch 2, Batch 800, Loss: 2.0205204379558563\n",
            "Epoch 2, Batch 900, Loss: 1.9788512134552\n",
            "Epoch 3, Batch 100, Loss: 1.937507185935974\n",
            "Epoch 3, Batch 200, Loss: 1.9014288246631623\n",
            "Epoch 3, Batch 300, Loss: 1.856128112077713\n",
            "Epoch 3, Batch 400, Loss: 1.817152246236801\n",
            "Epoch 3, Batch 500, Loss: 1.7680376088619232\n",
            "Epoch 3, Batch 600, Loss: 1.7431406688690185\n",
            "Epoch 3, Batch 700, Loss: 1.691617387533188\n",
            "Epoch 3, Batch 800, Loss: 1.648192822933197\n",
            "Epoch 3, Batch 900, Loss: 1.6062095069885254\n",
            "Epoch 4, Batch 100, Loss: 1.5492055439949035\n",
            "Epoch 4, Batch 200, Loss: 1.4947233188152313\n",
            "Epoch 4, Batch 300, Loss: 1.4570604598522185\n",
            "Epoch 4, Batch 400, Loss: 1.4266467130184173\n",
            "Epoch 4, Batch 500, Loss: 1.391392798423767\n",
            "Epoch 4, Batch 600, Loss: 1.3453848326206208\n",
            "Epoch 4, Batch 700, Loss: 1.3210773515701293\n",
            "Epoch 4, Batch 800, Loss: 1.296860281229019\n",
            "Epoch 4, Batch 900, Loss: 1.2598725652694702\n",
            "Epoch 5, Batch 100, Loss: 1.2174409902095795\n",
            "Epoch 5, Batch 200, Loss: 1.1884145468473435\n",
            "Epoch 5, Batch 300, Loss: 1.1731873542070388\n",
            "Epoch 5, Batch 400, Loss: 1.1401234608888626\n",
            "Epoch 5, Batch 500, Loss: 1.1188052272796631\n",
            "Epoch 5, Batch 600, Loss: 1.096479331254959\n",
            "Epoch 5, Batch 700, Loss: 1.0585596394538879\n",
            "Epoch 5, Batch 800, Loss: 1.0429770290851592\n",
            "Epoch 5, Batch 900, Loss: 1.0534878474473954\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -836.4742817128822\n",
            "Epoch 1, Batch 200, Loss: -27012.77100097656\n",
            "Epoch 1, Batch 300, Loss: -162732.314140625\n",
            "Epoch 1, Batch 400, Loss: -512934.229375\n",
            "Epoch 1, Batch 500, Loss: -1175213.3025\n",
            "Epoch 1, Batch 600, Loss: -2231259.615\n",
            "Epoch 1, Batch 700, Loss: -3746366.5275\n",
            "Epoch 1, Batch 800, Loss: -5798551.5\n",
            "Epoch 1, Batch 900, Loss: -8463331.895\n",
            "Epoch 2, Batch 100, Loss: -13207543.09\n",
            "Epoch 2, Batch 200, Loss: -17450703.37\n",
            "Epoch 2, Batch 300, Loss: -22503447.16\n",
            "Epoch 2, Batch 400, Loss: -28342576.24\n",
            "Epoch 2, Batch 500, Loss: -35102740.74\n",
            "Epoch 2, Batch 600, Loss: -42487822.96\n",
            "Epoch 2, Batch 700, Loss: -51000520.96\n",
            "Epoch 2, Batch 800, Loss: -60225607.84\n",
            "Epoch 2, Batch 900, Loss: -70673022.92\n",
            "Epoch 3, Batch 100, Loss: -86084478.08\n",
            "Epoch 3, Batch 200, Loss: -98649101.92\n",
            "Epoch 3, Batch 300, Loss: -112389588.0\n",
            "Epoch 3, Batch 400, Loss: -126352055.92\n",
            "Epoch 3, Batch 500, Loss: -142293100.16\n",
            "Epoch 3, Batch 600, Loss: -158265746.24\n",
            "Epoch 3, Batch 700, Loss: -176190125.92\n",
            "Epoch 3, Batch 800, Loss: -195139268.64\n",
            "Epoch 3, Batch 900, Loss: -214098275.04\n",
            "Epoch 4, Batch 100, Loss: -243174261.28\n",
            "Epoch 4, Batch 200, Loss: -266933919.52\n",
            "Epoch 4, Batch 300, Loss: -289753529.6\n",
            "Epoch 4, Batch 400, Loss: -314984355.2\n",
            "Epoch 4, Batch 500, Loss: -339964712.32\n",
            "Epoch 4, Batch 600, Loss: -366593754.56\n",
            "Epoch 4, Batch 700, Loss: -395262189.12\n",
            "Epoch 4, Batch 800, Loss: -424465523.2\n",
            "Epoch 4, Batch 900, Loss: -456086989.44\n",
            "Epoch 5, Batch 100, Loss: -498876113.6\n",
            "Epoch 5, Batch 200, Loss: -532644567.68\n",
            "Epoch 5, Batch 300, Loss: -567260125.44\n",
            "Epoch 5, Batch 400, Loss: -602477146.24\n",
            "Epoch 5, Batch 500, Loss: -640755746.56\n",
            "Epoch 5, Batch 600, Loss: -679170278.4\n",
            "Epoch 5, Batch 700, Loss: -716728734.72\n",
            "Epoch 5, Batch 800, Loss: -758291033.6\n",
            "Epoch 5, Batch 900, Loss: -803443386.24\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -0.03251188689493574\n",
            "Epoch 1, Batch 200, Loss: -0.17564129069447518\n",
            "Epoch 1, Batch 300, Loss: -0.48099490746855733\n",
            "Epoch 1, Batch 400, Loss: -1.534530577659607\n",
            "Epoch 1, Batch 500, Loss: -9.267262105941773\n",
            "Epoch 1, Batch 600, Loss: -inf\n",
            "Epoch 1, Batch 700, Loss: nan\n",
            "Epoch 1, Batch 800, Loss: nan\n",
            "Epoch 1, Batch 900, Loss: nan\n",
            "Epoch 2, Batch 100, Loss: nan\n",
            "Epoch 2, Batch 200, Loss: nan\n",
            "Epoch 2, Batch 300, Loss: nan\n",
            "Epoch 2, Batch 400, Loss: nan\n",
            "Epoch 2, Batch 500, Loss: nan\n",
            "Epoch 2, Batch 600, Loss: nan\n",
            "Epoch 2, Batch 700, Loss: nan\n",
            "Epoch 2, Batch 800, Loss: nan\n",
            "Epoch 2, Batch 900, Loss: nan\n",
            "Epoch 3, Batch 100, Loss: nan\n",
            "Epoch 3, Batch 200, Loss: nan\n",
            "Epoch 3, Batch 300, Loss: nan\n",
            "Epoch 3, Batch 400, Loss: nan\n",
            "Epoch 3, Batch 500, Loss: nan\n",
            "Epoch 3, Batch 600, Loss: nan\n",
            "Epoch 3, Batch 700, Loss: nan\n",
            "Epoch 3, Batch 800, Loss: nan\n",
            "Epoch 3, Batch 900, Loss: nan\n",
            "Epoch 4, Batch 100, Loss: nan\n",
            "Epoch 4, Batch 200, Loss: nan\n",
            "Epoch 4, Batch 300, Loss: nan\n",
            "Epoch 4, Batch 400, Loss: nan\n",
            "Epoch 4, Batch 500, Loss: nan\n",
            "Epoch 4, Batch 600, Loss: nan\n",
            "Epoch 4, Batch 700, Loss: nan\n",
            "Epoch 4, Batch 800, Loss: nan\n",
            "Epoch 4, Batch 900, Loss: nan\n",
            "Epoch 5, Batch 100, Loss: nan\n",
            "Epoch 5, Batch 200, Loss: nan\n",
            "Epoch 5, Batch 300, Loss: nan\n",
            "Epoch 5, Batch 400, Loss: nan\n",
            "Epoch 5, Batch 500, Loss: nan\n",
            "Epoch 5, Batch 600, Loss: nan\n",
            "Epoch 5, Batch 700, Loss: nan\n",
            "Epoch 5, Batch 800, Loss: nan\n",
            "Epoch 5, Batch 900, Loss: nan\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 1.16366823554039\n",
            "Epoch 1, Batch 200, Loss: 0.5263442105054855\n",
            "Epoch 1, Batch 300, Loss: 0.44775204852223394\n",
            "Epoch 1, Batch 400, Loss: 0.41326446205377576\n",
            "Epoch 1, Batch 500, Loss: 0.38689208537340164\n",
            "Epoch 1, Batch 600, Loss: 0.3953733475506306\n",
            "Epoch 1, Batch 700, Loss: 0.36777890667319296\n",
            "Epoch 1, Batch 800, Loss: 0.35870844617486\n",
            "Epoch 1, Batch 900, Loss: 0.33734407685697076\n",
            "Epoch 2, Batch 100, Loss: 0.31834926895797255\n",
            "Epoch 2, Batch 200, Loss: 0.33263789154589174\n",
            "Epoch 2, Batch 300, Loss: 0.30816330917179585\n",
            "Epoch 2, Batch 400, Loss: 0.32403608471155165\n",
            "Epoch 2, Batch 500, Loss: 0.32021242924034593\n",
            "Epoch 2, Batch 600, Loss: 0.2974679523706436\n",
            "Epoch 2, Batch 700, Loss: 0.29175818860530855\n",
            "Epoch 2, Batch 800, Loss: 0.28321443669497964\n",
            "Epoch 2, Batch 900, Loss: 0.29470737725496293\n",
            "Epoch 3, Batch 100, Loss: 0.26599072106182575\n",
            "Epoch 3, Batch 200, Loss: 0.2725881849229336\n",
            "Epoch 3, Batch 300, Loss: 0.2766114109754562\n",
            "Epoch 3, Batch 400, Loss: 0.27935412995517256\n",
            "Epoch 3, Batch 500, Loss: 0.26635643996298314\n",
            "Epoch 3, Batch 600, Loss: 0.28298403330147265\n",
            "Epoch 3, Batch 700, Loss: 0.26172439120709895\n",
            "Epoch 3, Batch 800, Loss: 0.24895258862525224\n",
            "Epoch 3, Batch 900, Loss: 0.27398510321974756\n",
            "Epoch 4, Batch 100, Loss: 0.23557766228914262\n",
            "Epoch 4, Batch 200, Loss: 0.2560770381242037\n",
            "Epoch 4, Batch 300, Loss: 0.2629371690005064\n",
            "Epoch 4, Batch 400, Loss: 0.24611437022686006\n",
            "Epoch 4, Batch 500, Loss: 0.24412614077329636\n",
            "Epoch 4, Batch 600, Loss: 0.25607479639351366\n",
            "Epoch 4, Batch 700, Loss: 0.2492527712881565\n",
            "Epoch 4, Batch 800, Loss: 0.25165378041565417\n",
            "Epoch 4, Batch 900, Loss: 0.22637825176119805\n",
            "Epoch 5, Batch 100, Loss: 0.2343814913928509\n",
            "Epoch 5, Batch 200, Loss: 0.22046669580042363\n",
            "Epoch 5, Batch 300, Loss: 0.2363150219991803\n",
            "Epoch 5, Batch 400, Loss: 0.22509747073054315\n",
            "Epoch 5, Batch 500, Loss: 0.2239337517693639\n",
            "Epoch 5, Batch 600, Loss: 0.23347150787711143\n",
            "Epoch 5, Batch 700, Loss: 0.2259340538829565\n",
            "Epoch 5, Batch 800, Loss: 0.22072861783206463\n",
            "Epoch 5, Batch 900, Loss: 0.24613066740334033\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: 2.301633472442627\n",
            "Epoch 1, Batch 200, Loss: 2.2665812301635744\n",
            "Epoch 1, Batch 300, Loss: 2.2309450459480287\n",
            "Epoch 1, Batch 400, Loss: 2.198971016407013\n",
            "Epoch 1, Batch 500, Loss: 2.1588699316978452\n",
            "Epoch 1, Batch 600, Loss: 2.126855537891388\n",
            "Epoch 1, Batch 700, Loss: 2.0850780522823333\n",
            "Epoch 1, Batch 800, Loss: 2.0507929599285126\n",
            "Epoch 1, Batch 900, Loss: 2.011564118862152\n",
            "Epoch 2, Batch 100, Loss: 1.9514932608604432\n",
            "Epoch 2, Batch 200, Loss: 1.9129989922046662\n",
            "Epoch 2, Batch 300, Loss: 1.869720448255539\n",
            "Epoch 2, Batch 400, Loss: 1.8230649614334107\n",
            "Epoch 2, Batch 500, Loss: 1.784496899843216\n",
            "Epoch 2, Batch 600, Loss: 1.7470801103115081\n",
            "Epoch 2, Batch 700, Loss: 1.7044420742988586\n",
            "Epoch 2, Batch 800, Loss: 1.664617668390274\n",
            "Epoch 2, Batch 900, Loss: 1.6280299627780914\n",
            "Epoch 3, Batch 100, Loss: 1.5697081100940704\n",
            "Epoch 3, Batch 200, Loss: 1.5153146541118623\n",
            "Epoch 3, Batch 300, Loss: 1.4934891283512115\n",
            "Epoch 3, Batch 400, Loss: 1.4379773092269899\n",
            "Epoch 3, Batch 500, Loss: 1.4307171952724458\n",
            "Epoch 3, Batch 600, Loss: 1.3775243389606475\n",
            "Epoch 3, Batch 700, Loss: 1.3498208713531494\n",
            "Epoch 3, Batch 800, Loss: 1.3211738991737365\n",
            "Epoch 3, Batch 900, Loss: 1.2888011968135833\n",
            "Epoch 4, Batch 100, Loss: 1.2407244789600371\n",
            "Epoch 4, Batch 200, Loss: 1.2177537167072296\n",
            "Epoch 4, Batch 300, Loss: 1.1900504195690156\n",
            "Epoch 4, Batch 400, Loss: 1.1663151746988296\n",
            "Epoch 4, Batch 500, Loss: 1.13505444586277\n",
            "Epoch 4, Batch 600, Loss: 1.1108337044715881\n",
            "Epoch 4, Batch 700, Loss: 1.0798615938425065\n",
            "Epoch 4, Batch 800, Loss: 1.0793835431337357\n",
            "Epoch 4, Batch 900, Loss: 1.0448528391122818\n",
            "Epoch 5, Batch 100, Loss: 1.0222424006462096\n",
            "Epoch 5, Batch 200, Loss: 1.0055739051103592\n",
            "Epoch 5, Batch 300, Loss: 0.9793170666694642\n",
            "Epoch 5, Batch 400, Loss: 0.9658325707912445\n",
            "Epoch 5, Batch 500, Loss: 0.9414868092536927\n",
            "Epoch 5, Batch 600, Loss: 0.93361108481884\n",
            "Epoch 5, Batch 700, Loss: 0.9117275869846344\n",
            "Epoch 5, Batch 800, Loss: 0.9005251675844193\n",
            "Epoch 5, Batch 900, Loss: 0.8918994802236557\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -3.8445736878644676\n",
            "Epoch 1, Batch 200, Loss: -10.589019608497619\n",
            "Epoch 1, Batch 300, Loss: -16.99397102355957\n",
            "Epoch 1, Batch 400, Loss: -23.263654041290284\n",
            "Epoch 1, Batch 500, Loss: -29.48031280517578\n",
            "Epoch 1, Batch 600, Loss: -35.65614284515381\n",
            "Epoch 1, Batch 700, Loss: -41.826286506652835\n",
            "Epoch 1, Batch 800, Loss: -47.958692741394046\n",
            "Epoch 1, Batch 900, Loss: -54.11761447906494\n",
            "Epoch 2, Batch 100, Loss: -62.595487365722654\n",
            "Epoch 2, Batch 200, Loss: -68.70973358154296\n",
            "Epoch 2, Batch 300, Loss: -74.82784980773926\n",
            "Epoch 2, Batch 400, Loss: -80.9516707611084\n",
            "Epoch 2, Batch 500, Loss: -87.0648104095459\n",
            "Epoch 2, Batch 600, Loss: -93.17664131164551\n",
            "Epoch 2, Batch 700, Loss: -99.28858322143554\n",
            "Epoch 2, Batch 800, Loss: -105.39099639892578\n",
            "Epoch 2, Batch 900, Loss: -111.47798561096191\n",
            "Epoch 3, Batch 100, Loss: -119.91619132995605\n",
            "Epoch 3, Batch 200, Loss: -125.99752670288086\n",
            "Epoch 3, Batch 300, Loss: -132.11237564086915\n",
            "Epoch 3, Batch 400, Loss: -138.19305053710937\n",
            "Epoch 3, Batch 500, Loss: -144.3081234741211\n",
            "Epoch 3, Batch 600, Loss: -150.4052619934082\n",
            "Epoch 3, Batch 700, Loss: -156.49872314453125\n",
            "Epoch 3, Batch 800, Loss: -162.56991226196288\n",
            "Epoch 3, Batch 900, Loss: -168.66667907714844\n",
            "Epoch 4, Batch 100, Loss: -177.0939356994629\n",
            "Epoch 4, Batch 200, Loss: -183.16320983886717\n",
            "Epoch 4, Batch 300, Loss: -189.2655714416504\n",
            "Epoch 4, Batch 400, Loss: -195.3272085571289\n",
            "Epoch 4, Batch 500, Loss: -201.46941497802734\n",
            "Epoch 4, Batch 600, Loss: -207.5088330078125\n",
            "Epoch 4, Batch 700, Loss: -213.60380447387695\n",
            "Epoch 4, Batch 800, Loss: -219.66971099853515\n",
            "Epoch 4, Batch 900, Loss: -225.794755859375\n",
            "Epoch 5, Batch 100, Loss: -234.1533773803711\n",
            "Epoch 5, Batch 200, Loss: -240.29993118286131\n",
            "Epoch 5, Batch 300, Loss: -246.35930130004883\n",
            "Epoch 5, Batch 400, Loss: -252.45553817749024\n",
            "Epoch 5, Batch 500, Loss: -258.5389437866211\n",
            "Epoch 5, Batch 600, Loss: -264.61848541259764\n",
            "Epoch 5, Batch 700, Loss: -270.71353424072265\n",
            "Epoch 5, Batch 800, Loss: -276.8221704101563\n",
            "Epoch 5, Batch 900, Loss: -282.8543243408203\n",
            "Finished Training\n",
            "Epoch 1, Batch 100, Loss: -0.1826860544877127\n",
            "Epoch 1, Batch 200, Loss: -0.49622539937496185\n",
            "Epoch 1, Batch 300, Loss: -0.7815624964237213\n",
            "Epoch 1, Batch 400, Loss: -1.0756207191944123\n",
            "Epoch 1, Batch 500, Loss: -1.4166907644271851\n",
            "Epoch 1, Batch 600, Loss: -1.8055918169021608\n",
            "Epoch 1, Batch 700, Loss: -2.2166889154911043\n",
            "Epoch 1, Batch 800, Loss: -2.6883930587768554\n",
            "Epoch 1, Batch 900, Loss: -3.193834218978882\n",
            "Epoch 2, Batch 100, Loss: -3.926625916957855\n",
            "Epoch 2, Batch 200, Loss: -4.5018203163146975\n",
            "Epoch 2, Batch 300, Loss: -5.0730894708633425\n",
            "Epoch 2, Batch 400, Loss: -5.665265421867371\n",
            "Epoch 2, Batch 500, Loss: -6.277720031738281\n",
            "Epoch 2, Batch 600, Loss: -6.898870859146118\n",
            "Epoch 2, Batch 700, Loss: -7.535118098258972\n",
            "Epoch 2, Batch 800, Loss: -8.168565950393678\n",
            "Epoch 2, Batch 900, Loss: -8.781596755981445\n",
            "Epoch 3, Batch 100, Loss: -9.652532558441163\n",
            "Epoch 3, Batch 200, Loss: -10.30350604057312\n",
            "Epoch 3, Batch 300, Loss: -10.950311336517334\n",
            "Epoch 3, Batch 400, Loss: -11.590644111633301\n",
            "Epoch 3, Batch 500, Loss: -12.250248193740845\n",
            "Epoch 3, Batch 600, Loss: -12.892741136550903\n",
            "Epoch 3, Batch 700, Loss: -13.55241374015808\n",
            "Epoch 3, Batch 800, Loss: -14.20548851966858\n",
            "Epoch 3, Batch 900, Loss: -14.841981325149536\n",
            "Epoch 4, Batch 100, Loss: -15.74076202392578\n",
            "Epoch 4, Batch 200, Loss: -16.379102869033815\n",
            "Epoch 4, Batch 300, Loss: -17.07058177947998\n",
            "Epoch 4, Batch 400, Loss: -17.722925548553466\n",
            "Epoch 4, Batch 500, Loss: -18.33899568557739\n",
            "Epoch 4, Batch 600, Loss: -19.017044448852538\n",
            "Epoch 4, Batch 700, Loss: -19.667498722076417\n",
            "Epoch 4, Batch 800, Loss: -20.29065523147583\n",
            "Epoch 4, Batch 900, Loss: -20.915664348602295\n",
            "Epoch 5, Batch 100, Loss: -21.849022617340086\n",
            "Epoch 5, Batch 200, Loss: -22.491879081726076\n",
            "Epoch 5, Batch 300, Loss: -23.146776428222655\n",
            "Epoch 5, Batch 400, Loss: -23.80676990509033\n",
            "Epoch 5, Batch 500, Loss: -24.473719120025635\n",
            "Epoch 5, Batch 600, Loss: -25.12542106628418\n",
            "Epoch 5, Batch 700, Loss: -25.77017475128174\n",
            "Epoch 5, Batch 800, Loss: -26.420518798828127\n",
            "Epoch 5, Batch 900, Loss: -27.09291301727295\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "print(tabulate(hyperparam_list, headers = 'keys', tablefmt = 'psql'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylzpc_B_IiTf",
        "outputId": "80ccdd7f-ec0d-42a5-fb94-150ccc0edf3c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+----+------------------+-------------+--------------+----------------+-----------------------+\n",
            "|    | criterion        | optimizer   | activation   |   Dropout Rate | Accuracy              |\n",
            "|----+------------------+-------------+--------------+----------------+-----------------------|\n",
            "|  0 | CrossEntropyLoss | Adam        | Sigmoid      |            0.2 | {0.9614833333333334}  |\n",
            "|  1 | CrossEntropyLoss | SGD         | Sigmoid      |            0.2 | {0.11318333333333333} |\n",
            "| 22 | NLLLoss          | Adam        | Tanh         |            0.5 | {0.10385}             |\n",
            "| 21 | CrossEntropyLoss | SGD         | Tanh         |            0.5 | {0.8231}              |\n",
            "| 20 | CrossEntropyLoss | Adam        | Tanh         |            0.5 | {0.9482166666666667}  |\n",
            "| 19 | NLLLoss          | SGD         | ReLU         |            0.5 | {0.0992}              |\n",
            "| 18 | NLLLoss          | Adam        | ReLU         |            0.5 | {0.11318333333333333} |\n",
            "| 17 | CrossEntropyLoss | SGD         | ReLU         |            0.5 | {0.7953166666666667}  |\n",
            "| 16 | CrossEntropyLoss | Adam        | ReLU         |            0.5 | {0.9512333333333334}  |\n",
            "| 15 | NLLLoss          | SGD         | Sigmoid      |            0.5 | {0.11318333333333333} |\n",
            "| 14 | NLLLoss          | Adam        | Sigmoid      |            0.5 | {0.11318333333333333} |\n",
            "| 13 | CrossEntropyLoss | SGD         | Sigmoid      |            0.5 | {0.11318333333333333} |\n",
            "| 12 | CrossEntropyLoss | Adam        | Sigmoid      |            0.5 | {0.9535666666666667}  |\n",
            "| 11 | NLLLoss          | SGD         | Tanh         |            0.2 | {0.11318333333333333} |\n",
            "| 10 | NLLLoss          | Adam        | Tanh         |            0.2 | {0.47863333333333336} |\n",
            "|  9 | CrossEntropyLoss | SGD         | Tanh         |            0.2 | {0.8242}              |\n",
            "|  8 | CrossEntropyLoss | Adam        | Tanh         |            0.2 | {0.95595}             |\n",
            "|  7 | NLLLoss          | SGD         | ReLU         |            0.2 | {0.0992}              |\n",
            "|  6 | NLLLoss          | Adam        | ReLU         |            0.2 | {0.11318333333333333} |\n",
            "|  5 | CrossEntropyLoss | SGD         | ReLU         |            0.2 | {0.8298833333333333}  |\n",
            "|  4 | CrossEntropyLoss | Adam        | ReLU         |            0.2 | {0.965}               |\n",
            "|  3 | NLLLoss          | SGD         | Sigmoid      |            0.2 | {0.11318333333333333} |\n",
            "|  2 | NLLLoss          | Adam        | Sigmoid      |            0.2 | {0.11318333333333333} |\n",
            "| 23 | NLLLoss          | SGD         | Tanh         |            0.5 | {0.11318333333333333} |\n",
            "+----+------------------+-------------+--------------+----------------+-----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning for differnt parameter is as shown above.CrossEntropyloss has better accuracy than NLLLoss.\n",
        "\n",
        "Best Hyperparametrs: Criterion='CrossEntropyLoss', Optimizer='Adam', Activation=ReLU, Dropout=0.2"
      ],
      "metadata": {
        "id": "9pOfaTt72f3C"
      }
    }
  ]
}